{"posts":[{"title":"Andorid-databinding使用","text":"Andorid-databinding1. 在项目build.gradle文件( app文件夹下面的那个)中添加123456android { ...... buildFeatures { dataBinding = true }} 2. 修改activity_main.xml文件格式为data binding layout 点击activity_main.xml文件的code选项，查看代码，并将鼠标放在左上角，会弹出一个灯泡形状的提示框； 点击提示框，选择Convert to data binding layout 代码格式修改完成 12345678910111213141516&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;layout xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:app=\"http://schemas.android.com/apk/res-auto\" xmlns:tools=\"http://schemas.android.com/tools\"&gt; &lt;data&gt; &lt;/data&gt; &lt;androidx.constraintlayout.widget.ConstraintLayout android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" tools:context=\".MainActivity\"&gt; &lt;/androidx.constraintlayout.widget.ConstraintLayout&gt;&lt;/layout&gt; 3. 在activity_main.xml中添加一个TextView控件12345678910111213141516171819202122232425&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;layout xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:app=\"http://schemas.android.com/apk/res-auto\" xmlns:tools=\"http://schemas.android.com/tools\"&gt; &lt;data&gt; &lt;/data&gt; &lt;androidx.constraintlayout.widget.ConstraintLayout android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" tools:context=\".MainActivity\"&gt; &lt;TextView android:id=\"@+id/textView\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:text=\"TextView\" app:layout_constraintBottom_toBottomOf=\"parent\" app:layout_constraintEnd_toEndOf=\"parent\" app:layout_constraintStart_toStartOf=\"parent\" app:layout_constraintTop_toTopOf=\"parent\" /&gt; &lt;/androidx.constraintlayout.widget.ConstraintLayout&gt;&lt;/layout&gt; 4. 在MainActivity.kt文件中修改修改完activity_main.xml的格式后，系统会为这个布局文件自动创建一个绑定类ActivityMainBinding 123456789101112131415161718package com.example.a1_data_bindingimport androidx.appcompat.app.AppCompatActivityimport android.os.Bundleimport androidx.databinding.DataBindingUtilimport com.example.a1_data_binding.databinding.ActivityMainBindingclass MainActivity : AppCompatActivity() { private lateinit var binding: ActivityMainBinding override fun onCreate(savedInstanceState: Bundle?) { super.onCreate(savedInstanceState) binding = DataBindingUtil.setContentView(this, R.layout.activity_main) // 通过binding.使用布局中的控件 binding.textView.text = \"DataBinding\" }}","link":"/2022/10/12/Andorid-databinding%E4%BD%BF%E7%94%A8/"},{"title":"Andorid-okHttp","text":"Android-okhttp1. 添加依赖1234implementation 'com.squareup.okhttp3:okhttp:4.10.0'implementation 'com.google.code.gson:gson:2.8.9'// https://mvnrepository.com/artifact/com.squareup.picasso/picassoimplementation 'com.squareup.picasso:picasso:2.5.2' 2. activity_main.xml12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;androidx.constraintlayout.widget.ConstraintLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:app=\"http://schemas.android.com/apk/res-auto\" xmlns:tools=\"http://schemas.android.com/tools\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" tools:context=\".MainActivity\"&gt; &lt;ImageView android:id=\"@+id/imageView\" android:layout_width=\"411dp\" android:layout_height=\"314dp\" android:contentDescription=\"@string/image\" app:layout_constraintBottom_toBottomOf=\"parent\" app:layout_constraintStart_toStartOf=\"parent\" app:layout_constraintTop_toTopOf=\"parent\" app:layout_constraintVertical_bias=\"0.146\" tools:srcCompat=\"@tools:sample/backgrounds/scenic\" /&gt; &lt;Button android:id=\"@+id/button\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:text=\"@string/button\" app:layout_constraintBottom_toBottomOf=\"parent\" app:layout_constraintEnd_toEndOf=\"parent\" app:layout_constraintHorizontal_bias=\"0.498\" app:layout_constraintStart_toStartOf=\"parent\" app:layout_constraintTop_toTopOf=\"parent\" app:layout_constraintVertical_bias=\"0.672\" /&gt; &lt;ProgressBar android:id=\"@+id/progressBar\" style=\"?android:attr/progressBarStyle\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" app:layout_constraintBottom_toBottomOf=\"parent\" app:layout_constraintEnd_toEndOf=\"parent\" app:layout_constraintHorizontal_bias=\"0.498\" app:layout_constraintStart_toStartOf=\"parent\" app:layout_constraintTop_toTopOf=\"parent\" app:layout_constraintVertical_bias=\"0.3\" /&gt;&lt;/androidx.constraintlayout.widget.ConstraintLayout&gt; 3. okHttp工具类HttpUtil.kt1234567891011121314151617package com.example.a14_okhttpimport okhttp3.OkHttpClientimport okhttp3.Requestimport java.net.HttpURLConnectionimport java.net.URLobject HttpUtil { fun sendHttpRequest(method: String, address: String, callback: okhttp3.Callback) { val client = OkHttpClient() val request = Request.Builder() .url(address) .build() client.newCall(request).enqueue(callback) }} 4. ImageData.kt数据类123package com.example.a14_okhttpclass ImageData (val code:String, val imgurl: String, val width: String, val height: String) 5. MainActivity.kt12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package com.example.a14_okhttpimport androidx.appcompat.app.AppCompatActivityimport android.os.Bundleimport android.util.Logimport android.view.Viewimport com.android.volley.Responseimport com.android.volley.toolbox.StringRequestimport com.android.volley.toolbox.Volleyimport com.example.a14_okhttp.databinding.ActivityMainBindingimport com.google.gson.Gsonimport com.squareup.picasso.Picassoimport okhttp3.Callimport okhttp3.Callbackimport okhttp3.OkHttpClientimport okhttp3.Requestimport java.io.IOExceptionimport java.lang.reflect.Array.getimport kotlin.concurrent.threadclass MainActivity : AppCompatActivity() { private lateinit var binding: ActivityMainBinding override fun onCreate(savedInstanceState: Bundle?) { super.onCreate(savedInstanceState) binding = ActivityMainBinding.inflate(layoutInflater) setContentView(binding.root) binding.progressBar.visibility = View.INVISIBLE binding.button.setOnClickListener { binding.progressBar.visibility = View.VISIBLE senRequestWithOkHttp() } } private fun senRequestWithOkHttp() { val address = \"https://api.ixiaowai.cn/api/api.php?return=json\" HttpUtil.sendHttpRequest(\"get\", address, object : Callback { override fun onResponse(call: Call, response: okhttp3.Response) { val responseData = response.body?.string() if (responseData != null) { showResponse(responseData) } } override fun onFailure(call: Call, e: IOException) { e.printStackTrace() } }) } private fun showResponse(response: String) { runOnUiThread { val gson = Gson() val image = gson.fromJson(response, ImageData::class.java) Picasso.with(applicationContext).load(image.imgurl).into(binding.imageView) binding.progressBar.visibility = View.INVISIBLE } }}","link":"/2022/10/12/Andorid-okHttp/"},{"title":"Android-AndroidStudio控制台乱码","text":"Android-AndroidStudio控制台乱码进入路径C:\\Program Files\\Android\\Android Studio\\bin下， 修改studio64.exe.vmoptions 文件，在最后一行添加 -Dfile.encoding=UTF-8， 重启AndroidStudio。","link":"/2022/10/12/Android-AndroidStudio%E6%8E%A7%E5%88%B6%E5%8F%B0%E4%B9%B1%E7%A0%81/"},{"title":"Andorid-底部导航栏","text":"Android-底部导航栏1. 创建三个Fragment| – demo_bottom_navigation | – – MainActivity.kt | – – ui | – – – firstPage | – – – – FirstFragment.kt | – – – – FirstViewModel.kt | – – – secondPage | – – – – SecondFragment.kt | – – – – SecondViewModel.kt | – – – thirdPage | – – – –ThirdFragment.kt | – – – – ThirdViewModel.kt 2. 添加导航 创建一个navigation资源文件，将三个fragment的xml文件添加进来， 修改id属性 1234567891011121314151617181920212223&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;navigation xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:app=\"http://schemas.android.com/apk/res-auto\" xmlns:tools=\"http://schemas.android.com/tools\" android:id=\"@+id/navigation\" app:startDestination=\"@id/firstPage\"&gt; &lt;fragment android:id=\"@+id/firstPage\" android:name=\"com.example.demo_bottom_navigation.ui.firstPage.FirstFragment\" android:label=\"fragment_first\" tools:layout=\"@layout/fragment_first\" /&gt; &lt;fragment android:id=\"@+id/secondPage\" android:name=\"com.example.demo_bottom_navigation.ui.secondPage.SecondFragment\" android:label=\"fragment_second\" tools:layout=\"@layout/fragment_second\" /&gt; &lt;fragment android:id=\"@+id/thirdPage\" android:name=\"com.example.demo_bottom_navigation.ui.thirdPage.ThirdFragment\" android:label=\"fragment_third\" tools:layout=\"@layout/fragment_third\" /&gt;&lt;/navigation&gt; 3. 修改activity_main.xml文件 添加一个BottomNavigationView控件和TextView控件， 通过convert view将TextView修改为fragment， 添加布局 123456789101112131415161718192021222324252627282930313233&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;androidx.constraintlayout.widget.ConstraintLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:app=\"http://schemas.android.com/apk/res-auto\" xmlns:tools=\"http://schemas.android.com/tools\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" tools:context=\".MainActivity\"&gt; &lt;com.google.android.material.bottomnavigation.BottomNavigationView android:id=\"@+id/nav_view\" android:layout_width=\"match_parent\" android:layout_height=\"wrap_content\" app:layout_constraintBottom_toBottomOf=\"parent\" app:layout_constraintEnd_toEndOf=\"parent\" android:background=\"?android:attr/windowBackground\" app:layout_constraintStart_toStartOf=\"parent\" app:menu=\"@menu/nav_bottom_menu\"&gt; &lt;/com.google.android.material.bottomnavigation.BottomNavigationView&gt; &lt;fragment android:id=\"@+id/nav_host_fragment_activity_main\" android:name=\"androidx.navigation.fragment.NavHostFragment\" app:navGraph=\"@navigation/navigation\" app:defaultNavHost=\"true\" android:layout_width=\"0dp\" android:layout_height=\"0dp\" app:layout_constraintBottom_toTopOf=\"@+id/nav_view\" app:layout_constraintEnd_toEndOf=\"parent\" app:layout_constraintStart_toStartOf=\"parent\" app:layout_constraintTop_toTopOf=\"parent\" /&gt;&lt;/androidx.constraintlayout.widget.ConstraintLayout&gt; 4. 创建菜单 创建一个menu类型的资源文件， 添加三个Menu Item， 修改id，title，图标属性 Menu Item的id需要与navigation的id一一对应 12345678910111213141516&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;menu xmlns:android=\"http://schemas.android.com/apk/res/android\"&gt; &lt;item android:id=\"@+id/firstPage\" android:icon=\"@drawable/ic_baseline_filter_1_24\" android:title=\"@string/firstpage\" /&gt; &lt;item android:id=\"@+id/secondPage\" android:icon=\"@drawable/ic_baseline_filter_2_24\" android:title=\"@string/secondpage\" /&gt; &lt;item android:id=\"@+id/thirdPage\" android:icon=\"@drawable/ic_baseline_filter_3_24\" android:title=\"@string/thirdpage\" /&gt;&lt;/menu&gt; 5. 修改MainActivity.kt，将底部导航栏和fragment绑定起来12345678910111213141516171819202122232425262728293031323334353637package com.example.demo_bottom_navigationimport androidx.appcompat.app.AppCompatActivityimport android.os.Bundleimport androidx.navigation.findNavControllerimport androidx.navigation.ui.AppBarConfigurationimport androidx.navigation.ui.setupActionBarWithNavControllerimport androidx.navigation.ui.setupWithNavControllerimport com.example.demo_bottom_navigation.databinding.ActivityMainBindingimport com.google.android.material.bottomnavigation.BottomNavigationViewclass MainActivity : AppCompatActivity() { private lateinit var binding: ActivityMainBinding override fun onCreate(savedInstanceState: Bundle?) { super.onCreate(savedInstanceState) // databinding binding = ActivityMainBinding.inflate(layoutInflater) setContentView(binding.root) // 将底部导航栏的点击事件与fragment的切换结合起来 val navView: BottomNavigationView = binding.navView // 通过findNavController找到activity中的fragment val navController = findNavController(R.id.nav_host_fragment_activity_main) // 添加底部导航栏的跳转逻辑 // navigation中的id需要和menu中的id一一对应 val appBarConfiguration = AppBarConfiguration( setOf( R.id.firstPage, R.id.secondPage, R.id.thirdPage ) ) setupActionBarWithNavController(navController, appBarConfiguration) navView.setupWithNavController(navController) }}","link":"/2022/10/12/Andorid-%E5%BA%95%E9%83%A8%E5%AF%BC%E8%88%AA%E6%A0%8F/"},{"title":"Android-UI常用控件","text":"Android-UI常用控件1. strings.xml12345678910111213141516&lt;resources&gt; &lt;string name=\"app_name\"&gt;5_UIDemo&lt;/string&gt; &lt;string name=\"textview\"&gt;显示&lt;/string&gt; &lt;string name=\"button1\"&gt;左&lt;/string&gt; &lt;string name=\"button2\"&gt;右&lt;/string&gt; &lt;string name=\"switch1\"&gt;开关&lt;/string&gt; &lt;string name=\"editText\"&gt;请输入百分比&lt;/string&gt; &lt;string name=\"button3\"&gt;确定&lt;/string&gt; &lt;string name=\"radiobutton1\"&gt;Android&lt;/string&gt; &lt;string name=\"radiobutton2\"&gt;apple&lt;/string&gt; &lt;string name=\"imageView\"&gt;图片&lt;/string&gt; &lt;string name=\"checkBoxText1\"&gt;内容1&lt;/string&gt; &lt;string name=\"checkBoxText2\"&gt;内容2&lt;/string&gt; &lt;string name=\"checkBoxText3\"&gt;内容3&lt;/string&gt; &lt;string name=\"ratingBar\"&gt;评价&lt;/string&gt;&lt;/resources&gt; 2. activity_main.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;androidx.constraintlayout.widget.ConstraintLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:app=\"http://schemas.android.com/apk/res-auto\" xmlns:tools=\"http://schemas.android.com/tools\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" tools:context=\".MainActivity\"&gt; &lt;TextView android:id=\"@+id/textView\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:layout_marginTop=\"32dp\" android:text=\"@string/textview\" android:textSize=\"34sp\" app:layout_constraintEnd_toEndOf=\"parent\" app:layout_constraintStart_toStartOf=\"parent\" app:layout_constraintTop_toTopOf=\"parent\" /&gt; &lt;androidx.constraintlayout.widget.Guideline android:id=\"@+id/guideline4\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:orientation=\"vertical\" app:layout_constraintGuide_begin=\"205dp\" /&gt; &lt;androidx.constraintlayout.widget.Guideline android:id=\"@+id/guideline5\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:orientation=\"horizontal\" app:layout_constraintGuide_percent=\"0.17\" /&gt; &lt;Button android:id=\"@+id/button1\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:text=\"@string/button1\" app:layout_constraintBottom_toTopOf=\"@+id/guideline5\" app:layout_constraintEnd_toStartOf=\"@+id/guideline4\" app:layout_constraintStart_toStartOf=\"parent\" app:layout_constraintTop_toTopOf=\"@+id/guideline5\" tools:ignore=\"DuplicateSpeakableTextCheck\" /&gt; &lt;Button android:id=\"@+id/button2\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:layout_marginEnd=\"24dp\" android:text=\"@string/button2\" app:layout_constraintBottom_toTopOf=\"@+id/guideline5\" app:layout_constraintEnd_toEndOf=\"parent\" app:layout_constraintHorizontal_bias=\"0.579\" app:layout_constraintStart_toStartOf=\"@+id/guideline4\" app:layout_constraintTop_toTopOf=\"@+id/guideline5\" /&gt; &lt;androidx.constraintlayout.widget.Guideline android:id=\"@+id/guideline6\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:orientation=\"horizontal\" app:layout_constraintGuide_percent=\"0.25\" /&gt; &lt;androidx.appcompat.widget.SwitchCompat android:id=\"@+id/switch1\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:minHeight=\"48dp\" android:text=\"@string/switch1\" app:layout_constraintBottom_toTopOf=\"@+id/guideline6\" app:layout_constraintEnd_toEndOf=\"parent\" app:layout_constraintStart_toStartOf=\"parent\" app:layout_constraintTop_toTopOf=\"@+id/guideline6\" /&gt; &lt;androidx.constraintlayout.widget.Guideline android:id=\"@+id/guideline7\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:orientation=\"horizontal\" app:layout_constraintGuide_begin=\"235dp\" /&gt; &lt;ProgressBar android:id=\"@+id/progressBar1\" style=\"?android:attr/progressBarStyle\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" app:layout_constraintBottom_toTopOf=\"@+id/guideline7\" app:layout_constraintEnd_toStartOf=\"@+id/guideline4\" app:layout_constraintStart_toStartOf=\"parent\" app:layout_constraintTop_toTopOf=\"@+id/guideline7\" /&gt; &lt;ProgressBar android:id=\"@+id/progressBar2\" style=\"?android:attr/progressBarStyleHorizontal\" android:layout_width=\"0dp\" android:layout_height=\"wrap_content\" android:indeterminate=\"true\" app:layout_constraintBottom_toTopOf=\"@+id/guideline7\" app:layout_constraintEnd_toEndOf=\"parent\" app:layout_constraintStart_toStartOf=\"@+id/guideline4\" app:layout_constraintTop_toTopOf=\"@+id/guideline7\" /&gt; &lt;androidx.constraintlayout.widget.Guideline android:id=\"@+id/guideline8\" android:layout_width=\"0dp\" android:layout_height=\"wrap_content\" android:orientation=\"horizontal\" app:layout_constraintGuide_begin=\"295dp\" /&gt; &lt;ProgressBar android:id=\"@+id/progressBar3\" style=\"?android:attr/progressBarStyleHorizontal\" android:layout_width=\"0dp\" android:layout_height=\"wrap_content\" app:layout_constraintBottom_toTopOf=\"@+id/guideline8\" app:layout_constraintEnd_toEndOf=\"parent\" app:layout_constraintStart_toStartOf=\"parent\" app:layout_constraintTop_toTopOf=\"@+id/guideline8\" tools:indeterminateTint=\"@color/purple_700\" /&gt; &lt;androidx.constraintlayout.widget.Guideline android:id=\"@+id/guideline9\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:orientation=\"horizontal\" app:layout_constraintGuide_begin=\"347dp\" /&gt; &lt;EditText android:id=\"@+id/editTextNumber\" android:layout_width=\"137dp\" android:layout_height=\"48dp\" android:autofillHints=\"\" android:ems=\"10\" android:hint=\"@string/editText\" android:inputType=\"number\" android:textColorHint=\"#757575\" app:layout_constraintBottom_toTopOf=\"@+id/guideline9\" app:layout_constraintEnd_toStartOf=\"@+id/guideline4\" app:layout_constraintStart_toStartOf=\"parent\" app:layout_constraintTop_toTopOf=\"@+id/guideline9\" /&gt; &lt;Button android:id=\"@+id/button3\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:text=\"@string/button3\" app:layout_constraintBottom_toTopOf=\"@+id/guideline9\" app:layout_constraintEnd_toEndOf=\"parent\" app:layout_constraintStart_toStartOf=\"@+id/guideline4\" app:layout_constraintTop_toTopOf=\"@+id/guideline9\" /&gt; &lt;androidx.constraintlayout.widget.Guideline android:id=\"@+id/guideline10\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:orientation=\"horizontal\" app:layout_constraintGuide_percent=\"0.6\" /&gt; &lt;RadioGroup android:id=\"@+id/radioGroup\" android:layout_width=\"97dp\" android:layout_height=\"110dp\" app:layout_constraintBottom_toTopOf=\"@+id/guideline10\" app:layout_constraintEnd_toStartOf=\"@+id/guideline4\" app:layout_constraintStart_toStartOf=\"parent\" app:layout_constraintTop_toTopOf=\"@+id/guideline10\"&gt; &lt;RadioButton android:id=\"@+id/radioButton1\" android:layout_width=\"match_parent\" android:layout_height=\"wrap_content\" android:checked=\"true\" android:text=\"@string/radiobutton1\" tools:ignore=\"DuplicateSpeakableTextCheck\" /&gt; &lt;RadioButton android:id=\"@+id/radioButton2\" android:layout_width=\"match_parent\" android:layout_height=\"wrap_content\" android:minHeight=\"48dp\" android:text=\"@string/radiobutton2\" /&gt; &lt;/RadioGroup&gt; &lt;ImageView android:id=\"@+id/imageView\" android:layout_width=\"81dp\" android:layout_height=\"64dp\" android:contentDescription=\"@string/imageView\" app:layout_constraintBottom_toTopOf=\"@+id/guideline10\" app:layout_constraintEnd_toEndOf=\"parent\" app:layout_constraintStart_toStartOf=\"@+id/guideline4\" app:layout_constraintTop_toTopOf=\"@+id/guideline10\" app:srcCompat=\"@drawable/android\" /&gt; &lt;androidx.constraintlayout.widget.Guideline android:id=\"@+id/guideline11\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:orientation=\"horizontal\" app:layout_constraintGuide_begin=\"534dp\" /&gt; &lt;SeekBar android:id=\"@+id/seekBar\" android:layout_width=\"0dp\" android:layout_height=\"wrap_content\" app:layout_constraintBottom_toTopOf=\"@+id/guideline11\" app:layout_constraintEnd_toEndOf=\"parent\" app:layout_constraintStart_toStartOf=\"parent\" app:layout_constraintTop_toTopOf=\"@+id/guideline11\" /&gt; &lt;CheckBox android:id=\"@+id/checkBox1\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:text=\"@string/checkBoxText1\" app:layout_constraintBottom_toTopOf=\"@+id/guideline13\" app:layout_constraintEnd_toStartOf=\"@+id/guideline4\" app:layout_constraintStart_toStartOf=\"parent\" app:layout_constraintTop_toTopOf=\"@+id/guideline13\" /&gt; &lt;CheckBox android:id=\"@+id/checkBox2\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:text=\"@string/checkBoxText2\" app:layout_constraintBottom_toBottomOf=\"@+id/checkBox1\" app:layout_constraintEnd_toEndOf=\"parent\" app:layout_constraintStart_toStartOf=\"parent\" app:layout_constraintTop_toTopOf=\"@+id/checkBox1\" /&gt; &lt;CheckBox android:id=\"@+id/checkBox3\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:text=\"@string/checkBoxText3\" app:layout_constraintBottom_toBottomOf=\"@+id/checkBox2\" app:layout_constraintEnd_toEndOf=\"parent\" app:layout_constraintStart_toStartOf=\"@+id/guideline4\" app:layout_constraintTop_toTopOf=\"@+id/checkBox2\" /&gt; &lt;androidx.constraintlayout.widget.Guideline android:id=\"@+id/guideline13\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:orientation=\"horizontal\" app:layout_constraintGuide_begin=\"588dp\" /&gt; &lt;androidx.constraintlayout.widget.Guideline android:id=\"@+id/guideline14\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:orientation=\"horizontal\" app:layout_constraintGuide_begin=\"658dp\" /&gt; &lt;RatingBar android:id=\"@+id/ratingBar\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:contentDescription=\"@string/ratingBar\" app:layout_constraintBottom_toTopOf=\"@+id/guideline14\" app:layout_constraintEnd_toEndOf=\"parent\" app:layout_constraintStart_toStartOf=\"parent\" app:layout_constraintTop_toTopOf=\"@+id/guideline14\" /&gt;&lt;/androidx.constraintlayout.widget.ConstraintLayout&gt; 3. MainActivity.kt123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102package com.example.a5_uidemoimport android.annotation.SuppressLintimport androidx.appcompat.app.AppCompatActivityimport android.os.Bundleimport android.util.Logimport android.widget.SeekBarimport android.widget.Toastimport kotlinx.android.synthetic.main.activity_main.*class MainActivity : AppCompatActivity() { fun isNumeric(s: String): Boolean { return try { s.toDouble() true } catch (e: NumberFormatException) { false } } @SuppressLint(\"SetTextI18n\") override fun onCreate(savedInstanceState: Bundle?) { super.onCreate(savedInstanceState) setContentView(R.layout.activity_main) button1.setOnClickListener { textView.text = getString(R.string.button1) } button2.setOnClickListener { textView.text = \"右\" } switch1.setOnCheckedChangeListener { compoundButton, b -&gt; if (b) { textView.text = \"开\" } else { textView.text = \"关\" } } button3.setOnClickListener { if (isNumeric(editTextNumber.text.toString())) { val number = editTextNumber.text.toString().toInt() progressBar3.progress = number } } radioGroup.setOnCheckedChangeListener { radioGroup, i -&gt; if (i == R.id.radioButton1) { imageView.setImageResource(R.drawable.android) } else { imageView.setImageResource(R.drawable.apple) } } seekBar.setOnSeekBarChangeListener(object :SeekBar.OnSeekBarChangeListener{ override fun onProgressChanged(p0: SeekBar?, p1: Int, p2: Boolean) { //p0 表示是哪个seekbar（以下同理） p1是当前值 p2表示是否是用户改变的值 textView.text = p1.toString() } override fun onStartTrackingTouch(p0: SeekBar?) {//刚开始触摸的时候 } override fun onStopTrackingTouch(p0: SeekBar?) {//结束触摸 } }) var text1 = \"\" var text2 = \"\" var text3 = \"\" checkBox1.setOnCheckedChangeListener { compoundButton, b -&gt; text1 = if (b) { \"文本1\" } else { \"\" } textView.text = \"$text1 $text2 $text3\" } checkBox2.setOnCheckedChangeListener { compoundButton, b -&gt; text2 = if (b) { \"文本2\" } else { \"\" } textView.text = \"$text1 $text2 $text3\" } checkBox3.setOnCheckedChangeListener { compoundButton, b -&gt; text3 = if (b) { \"文本3\" } else { \"\" } textView.text = \"$text1 $text2 $text3\" } ratingBar.setOnRatingBarChangeListener { ratingBar, fl, b -&gt; Toast.makeText(this, fl.toString(), Toast.LENGTH_SHORT).show() } }}","link":"/2022/10/12/Android-UI%E5%B8%B8%E7%94%A8%E6%8E%A7%E4%BB%B6/"},{"title":"Android-ViewModel","text":"Android-ViewModel1. ViewModel和ViewModelFactoryViewModel类旨在以注重生命周期的方式存储和管理界面相关的数据。ViewModel类让数据可在发生屏幕旋转等配置更改后继续留存。 [https://developer.android.google.cn/topic/libraries/architecture/viewmodel?hl=zh-cn]: 在创建ViewModel时一般需要继承ViewModel类 123class MainViewModel: ViewModel() { ......} 通常情况下，ViewModel需要一些参数，但是只使用这种继承的方法，无法在构造函数中传递参数，这时，就需要用到ViewModelProvider.Factory 2. 代码示例（加法计算）2.1 在app文件夹下的build.gradle中添加123456android { ...... buildFeatures { dataBinding = true }} 2.2 修改activity_main.xml布局文件在activity_main.xml文件中添加两个TextView和一个Button控件，并将文件代码转换成databinding格式； 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;layout xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:app=\"http://schemas.android.com/apk/res-auto\" xmlns:tools=\"http://schemas.android.com/tools\"&gt; &lt;data&gt; &lt;/data&gt; &lt;androidx.constraintlayout.widget.ConstraintLayout android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" tools:context=\".MainActivity\"&gt; &lt;Button android:id=\"@+id/button\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:text=\"@string/button\" android:textSize=\"24sp\" app:layout_constraintBottom_toBottomOf=\"parent\" app:layout_constraintEnd_toEndOf=\"parent\" app:layout_constraintHorizontal_bias=\"0.498\" app:layout_constraintStart_toStartOf=\"parent\" app:layout_constraintTop_toTopOf=\"parent\" app:layout_constraintVertical_bias=\"0.703\" /&gt; &lt;EditText android:id=\"@+id/editText\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:autofillHints=\"\" android:ems=\"10\" android:hint=\"请输入数字\" android:inputType=\"number\" android:textColorHint=\"#78909C\" android:textSize=\"20sp\" app:layout_constraintBottom_toBottomOf=\"parent\" app:layout_constraintEnd_toEndOf=\"parent\" app:layout_constraintHorizontal_bias=\"0.497\" app:layout_constraintStart_toStartOf=\"parent\" app:layout_constraintTop_toTopOf=\"parent\" app:layout_constraintVertical_bias=\"0.213\" /&gt; &lt;TextView android:id=\"@+id/totalTextView\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:text=\"TextView\" android:textSize=\"24sp\" app:layout_constraintBottom_toBottomOf=\"parent\" app:layout_constraintEnd_toEndOf=\"parent\" app:layout_constraintStart_toStartOf=\"parent\" app:layout_constraintTop_toTopOf=\"parent\" /&gt; &lt;/androidx.constraintlayout.widget.ConstraintLayout&gt;&lt;/layout&gt; 2.3 创建MainActivityViewModel.kt123456789101112131415161718192021222324package com.example.a3_viewmodel_demo1import androidx.lifecycle.LiveDataimport androidx.lifecycle.MutableLiveDataimport androidx.lifecycle.ViewModel// MainActivityViewModel需要提供一个初始化参数class MainActivityViewModel(total: Int) : ViewModel() { // 为保护数据的安全性，创建_total和total // _total可以读取和修改，隐藏在当前的ViewModel中 // total只能读取，暴露给外部 private var _total = MutableLiveData&lt;Int&gt;() val total: LiveData&lt;Int&gt; get() = _total init { _total.value = total } fun setTotal(inputNum: Int) { _total.value = _total.value?.plus(inputNum) }} 2.4 创建MainActivityViewModelFactor.kt1234567891011121314151617package com.example.a3_viewmodel_demo1import androidx.lifecycle.ViewModelimport androidx.lifecycle.ViewModelProviderimport java.lang.IllegalArgumentException// 传入MainActivityViewModel初始化的参数total// 继承自ViewModelProvider.Factoryclass MainActivityViewModelFactor(private val total: Int) : ViewModelProvider.Factory { override fun &lt;T : ViewModel?&gt; create(modelClass: Class&lt;T&gt;): T { if (modelClass.isAssignableFrom(MainActivityViewModel::class.java)) { return MainActivityViewModel(total) as T } throw IllegalArgumentException(\"unknown viewModel\") }} 2.5 修改MainActivity.kt12345678910111213141516171819202122232425262728293031323334package com.example.a3_viewmodel_demo1import androidx.appcompat.app.AppCompatActivityimport android.os.Bundleimport androidx.databinding.DataBindingUtilimport androidx.lifecycle.Observerimport androidx.lifecycle.ViewModelProviderimport com.example.a3_viewmodel_demo1.databinding.ActivityMainBindingclass MainActivity : AppCompatActivity() { private lateinit var binding: ActivityMainBinding private lateinit var viewModel: MainActivityViewModel private lateinit var viewModelFactor: MainActivityViewModelFactor override fun onCreate(savedInstanceState: Bundle?) { super.onCreate(savedInstanceState) binding = DataBindingUtil.setContentView(this, R.layout.activity_main) // 创建viewModelFactor viewModelFactor = MainActivityViewModelFactor(10) // 使用ViewModelProviderviewModel并通过viewModelFactor传入初始化参数 viewModel = ViewModelProvider(this, viewModelFactor).get(MainActivityViewModel::class.java) // 当ViewModel的total值发生变化时，调用该方法将计算的值显示在TextView上 viewModel.total.observe(this, Observer { binding.totalTextView.text = it.toString() }) // 监听button点击事件，当用户点击时，调用ViewModel的setTotal方法更新_total的值 binding.button.setOnClickListener { viewModel.setTotal(binding.editText.text.toString().toInt()) } }} 屏幕翻转数据不会丢失","link":"/2022/10/12/Android-ViewModel/"},{"title":"Android-android.app.Application cannot be cast to","text":"Android-android.app.Application cannot be cast to *在manifest/AndroidManifest.xml中添加 12&lt;application android:name=\" android:name=\"pakageName.ApplicationName\"&gt;","link":"/2022/10/12/Android-android.app.Application%20cannot%20be%20cast%20to/"},{"title":"Android-enableJetifier","text":"Android-enableJetifier12android.useAndroidX=trueandroid.enableJetifier=true AndroidX 是对 android.support.xxx 包的整理后产物。由于之前的 support 包过于混乱，所以，Google 推出了AndroidX android.enableJetifier=true 表示将依赖包也迁移到androidx 。如果取值为false,表示不迁移依赖包到androidx，但在使用依赖包中的内容时可能会出现问题，如果项目中没有使用任何三方依赖，可以设置为false。","link":"/2022/10/12/Android-enableJetifier/"},{"title":"CNN-VGG总结","text":"论文地址：https://arxiv.org/abs/1409.1556 1. VGG的主要贡献 VGG使用 2个3X3的卷积核 来代替 5X5的卷积核，3个3X3的卷积核 代替7X7的卷积核，在保证相同感受野的情况下，多个小卷积层堆积可以提升网络深度，增加特征提取能力； 相比于AlexNet，搭建了更深层次的神经网络。 2. VGG的主要结构 以最常用的VGG16为例： 自适应池化（AdaptiveAvgPool2d）:对输入信号，提供2维的自适应平均池化操作 对于任何输入大小的输入，可以将输出尺寸指定为H*W，但是输入和输出特征的数目不会变化。https://blog.csdn.net/qq_41997920/article/details/98963215 3. 模型代码及训练过程 模型文件下载地址：链接：https://pan.baidu.com/s/1h8r3_2HVqUKA_nIlXgOY5g提取码：kov6 3.1 模型代码及加载权重123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104\"\"\"当设置nn.Dropout(p=0.5, inplace=True)时,会出现后面的报错RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation:[torch.cuda.HalfTensor [16, 4096]], which is output 0 of ReluBackward1, is at version 2; expected version 1 instead.Hint: enable anomaly detection to find the operation that failed to compute its gradient,with torch.autograd.set_detect_anomaly(True).\"\"\"import torchimport torchvision.modelsfrom torch import nnvgg_types = { 'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], 'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], 'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'], 'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],}class VGG(nn.Module): def __init__(self, in_channels=3, num_classes=1000, vgg_type=\"VGG16\"): super(VGG, self).__init__() self.in_channels = in_channels self.features = self.create_conv_layers(vgg_types[vgg_type]) self.avgpool = nn.AdaptiveAvgPool2d(output_size=(7, 7)) self.classifier = nn.Sequential( nn.Linear(512 * 7 * 7, 4096, bias=True), nn.ReLU(inplace=True), nn.Dropout(p=0.5), nn.Linear(4096, 4096, bias=True), nn.ReLU(inplace=True), nn.Dropout(p=0.5), nn.Linear(4096, num_classes, bias=True) ) self.initialize_weights() def forward(self, x): x = self.features(x) x = self.avgpool(x) x = x.view(x.shape[0], -1) x = self.classifier(x) return x def create_conv_layers(self, architecture): layers = [] in_channels = self.in_channels for x in architecture: if type(x) == int: out_channels = x layers += [ nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), # 如果要使用官网的预训练权重，需要注释掉下面这句 nn.BatchNorm2d(x), nn.ReLU(inplace=True) ] in_channels = x elif x == 'M': layers += [nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))] return nn.Sequential(*layers) # 参数初始化 def initialize_weights(self): for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_uniform_(m.weight) if m.bias is not None: nn.init.constant_(m.bias, 0) elif isinstance(m, nn.BatchNorm2d): nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) elif isinstance(m, nn.Linear): nn.init.kaiming_uniform_(m.weight) nn.init.constant_(m.bias, 0)def get_vgg_model(in_channels=3, num_classes=1000, vgg_type=\"VGG16\", is_load_weights=False, model_path=None): vgg_type = vgg_type.upper() if is_load_weights: assert model_path is not None, \"model path cannot be none\" device = \"cuda\" if torch.cuda.is_available() else \"cpu\" model = VGG(in_channels=in_channels, num_classes=10, vgg_type=vgg_type) missing_keys, unexpected_keys = model.load_state_dict(torch.load(model_path, map_location=device), strict=False) print(f\"missing keys: {missing_keys}\") print(f\"unexpected key: {unexpected_keys}\") model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes) else: model = VGG(in_channels=in_channels, num_classes=num_classes, vgg_type=vgg_type) return modelif __name__ == '__main__': model_path = \"./VGG16_0.898.pth\" model = get_vgg_model(num_classes=10, is_load_weights=True, model_path=model_path) input_x = torch.randn((3, 3, 224, 224)) preds = model(input_x) print(preds.shape) 由于原始的VGG模型中没有BN层，所以使用上述代码加载Pytorch官方的预训练权重会出错；如果要使用官方的预训练模型，需要注释掉BN层，并将第89行的num_classes=10修改为num_classes=1000；VGG16_0.898.pth模型文件是我用上述网络结构在CIFAR10数据集训练的权重，可以作为练习使用。 3.2 使用CIFAR10数据集训练模型123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116import torchfrom torch.utils.data import Dataset, DataLoaderfrom torch.utils.tensorboard import SummaryWriterfrom torch import optim, nn, cudaimport torchvisionfrom torchvision import transformsfrom model import get_vgg_modelimport platformfrom tqdm import tqdmimport numpy as npimport requestsimport ossysstr = platform.system()device = \"cuda\" if torch.cuda.is_available() else \"cpu\"num_workers = 0 if sysstr == \"Windows\" else 8transform = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])])classes = (\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")train_dataset = torchvision.datasets.CIFAR10(\"./data/\", train=True, download=True, transform=transform)val_dataset = torchvision.datasets.CIFAR10(\"./data/\", train=False, download=True, transform=transform)batch_size = 64train_dl = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)val_dl = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers)logs_dir = \"/root/tf-logs/\"writer = SummaryWriter(logs_dir + \"vgg\")writer_dict = {}for i in range(10): writer_dict[f\"writer_{i}\"] = SummaryWriter(logs_dir + classes[i])vgg_type = \"VGG16\"model = get_vgg_model(vgg_type=vgg_type, num_classes=10)num_epochs = 100loss_fn = nn.CrossEntropyLoss().to(device)optimizer = optim.Adam(model.parameters(), lr=1e-4)# 混合精度计算scaler = cuda.amp.GradScaler()torch.backends.cudnn.benchmark = Truebest_model_acc = 0model.to(device)for epoch in range(num_epochs): train_loop = tqdm(train_dl, leave=True, desc=f\"Train Epoch: {epoch}/{num_epochs}\") model.train() epoch_loss = 0.0 for step, (imgs, labels) in enumerate(train_loop): imgs = imgs.to(device) labels = labels.to(device) with cuda.amp.autocast(): preds = model(imgs) loss = loss_fn(preds, labels) optimizer.zero_grad() scaler.scale(loss).backward() scaler.step(optimizer) scaler.update() epoch_loss += loss.item() train_loop.set_postfix(loss=f\"{np.round(loss.item(), 3)}\") epoch_loss /= len(train_loop) # 模型的测试 model.eval() acc_num = 0 # 每个类别的分类正确的个数 class_correct = list(0. for i in range(10)) # 类别数 class_total = list(0. for i in range(10)) with torch.no_grad(): val_loop = tqdm(val_dl, leave=True, desc=f\"Val: \") for step, (imgs, labels) in enumerate(val_loop): mini_batch_size = len(imgs) imgs = imgs.to(device) labels = labels.to(device) output = model(imgs) _, preds = torch.max(output, 1) c = (preds == labels) c = c.squeeze() for i in range(mini_batch_size): label = labels[i] # 如果预测正确，则相应类别预测正确的数目加1（true） # 如果预测错误，则相应类别预测正确的数目加0（false） class_correct[label] += c[i] class_total[label] += 1 # 总的分类准确率 acc = c.sum().item() acc_num += acc accuracy = acc_num / len(val_dataset) for i in range(10): # tqdm.write( # f\"Accuracy of {classes[i]:&gt;10} : \" # f\"{np.round(100 * class_correct[i].item() / class_total[i], 2)}%\") writer_dict[f\"writer_{i}\"].add_scalar(f\"classes acc\", class_correct[i].item() / class_total[i], epoch) # 保存模型 if accuracy &gt; best_model_acc: torch.save(model.state_dict(), f\"./vgg_{vgg_type}_{np.round(accuracy, 3)}.pth\") best_model_acc = accuracy tqdm.write(f\"Val Accuracy: {np.round(accuracy * 100, 2)}%\") writer.add_scalar(\"classes acc\", accuracy, epoch) writer.add_scalar(\"loss\", epoch_loss, epoch)os.system(\"shutdown\") 3.3 训练结果","link":"/2022/10/12/CNN-VGG%E6%80%BB%E7%BB%93/"},{"title":"Git常见问题-Deployer not found git","text":"1npm install hexo-deployer-git --save","link":"/2022/10/12/Git%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98-Deployer-not-found-git/"},{"title":"GAN-Condition GAN","text":"论文地址：https://arxiv.org/abs/1411.1784 1. 提出的背景 传统的GAN虽然可以生成图像，但是无法控制具体生成图像的种类。例如在生成手写体时，GAN和DCGAN都可以生成0-9这十个数字，但是用户无法指定具体生成那个数字的图像； GAN和DCGAN存在模式崩塌现象(Mode collapse(模式坍塌))。 2. 主要思想 GAN主要包括两个网络，一个生成器和一个判别器，GAN的主要优化函数是 $\\begin{array}{c}\\min _{G} \\max _{D} V(D, G)=\\mathbb{E} x \\sim p d a t a(x)[\\log D(x)] \\+\\mathbb{E} z \\sim p d a t a(z)[\\log (1-D(G(z)))]\\end{array}$CGAN与传统的GAN相比，区别就是增加了标签作为训练的一个输入，CGAN的优化函数为$\\min {G} \\max {D} V(D, G)=\\mathbb{E}{x \\sim p \\text { data }(x)}[\\log D(x \\mid y)]+\\mathbb{E}{z \\sim p \\text { data }(z)}[\\log (1-D(G(z \\mid y)))]$ 结构图： 3. 具体实现 生成器 123456789101112131415161718192021222324252627282930313233343536373839class Generator(nn.Module): def __init__(self): super(Generator, self).__init__() # 两个128 x 7 x 7 cat后依然为256 x 7 x 7 self.linear1 = nn.Sequential( nn.Linear(100, 128 * 7 * 7), nn.ReLU(), nn.BatchNorm1d(128 * 7 * 7), ) self.linear2 = nn.Sequential( nn.Linear(10, 128 * 7 * 7), nn.ReLU(), nn.BatchNorm1d(128 * 7 * 7), ) self.model = nn.Sequential( # 128 x 7 x 7 nn.ConvTranspose2d(256, 128, kernel_size=3, stride=1, padding=1), nn.ReLU(), nn.BatchNorm2d(128), # 64 x 14 x 14 nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1), nn.ReLU(), nn.BatchNorm2d(64), # 1 x 28 x 28 nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1), nn.Tanh() ) def forward(self, x, c): x = self.linear1(x) x = x.view(-1, 128, 7, 7) c = self.linear2(c) c = c.view(-1, 128, 7, 7) # 256 x 7 x 7 # 在channels方面合并 x = torch.cat([x, c], dim=1) return self.model(x) 判别器接收两个输入，一个是随机噪声，一个是标签，将噪声和标签转换为长度128x7x7的向量，再将两个向量连接起来，构成一个256x7x7的向量，再进行三次的转置卷积，最终输出一个1x28x28（与mnist数据集的大小保持一致）的图像。 判别器 12345678910111213141516171819202122232425262728293031class Discriminator(nn.Module): def __init__(self): super(Discriminator, self).__init__() # input: 1 x 28 x 28 + 10 condition self.linear = nn.Sequential( nn.Linear(10, 1 * 28 * 28), nn.ReLU() ) self.model = nn.Sequential( nn.Conv2d(2, 64, kernel_size=3, stride=2), nn.LeakyReLU(), nn.Dropout(p=0.3), nn.Conv2d(64, 128, kernel_size=3, stride=2), nn.LeakyReLU(), nn.Dropout(p=0.3), nn.BatchNorm2d(128), ) self.fc = nn.Sequential( nn.Linear(128 * 6 * 6, 1), nn.Sigmoid() ) def forward(self, x, c): c = self.linear(c) c = c.view(-1, 1, 28, 28) # 2 x 28 x 28 x = torch.cat([x, c], dim=1) x = self.model(x) x = x.view(-1, 128 * 6 * 6) x = self.fc(x) return x 判别器也是接收两个参数，一个是图像（可能是真实图像，也可能是生成的虚假的图像），另一个是标签，首先将标签转换为1x28x28的形状，然后将这个向量和图像连接起来，构成一个2x28x28的向量，最后经过卷积、激活、池化、线形层输出一个结果（真或者假）。 训练 训练判别器 判别器要尽可能地区分出真实图片和虚假的图片；将真实的图像和标签放入到判别器中，计算判别器输出与1之间的损失；根据噪声生成虚假的图片，将虚假的图片和标签放入到判别器中，计算判别器输出和0之间的损失；反向传播、迭代优化。 训练生成器 生成器要尽可能的使生成的图像接近真实的图像，让判别器无法判断出图片的来源（真实还是生成）；将生成的虚假的图片放入到判别器中，计算判别器的输出与1之间的损失；反向传播、迭代优化。 4. 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169import torchfrom torch import nn, cudafrom torch.utils import datafrom torch.utils.tensorboard import SummaryWriterimport torchvisionfrom torchvision import transformsimport numpy as npfrom tqdm import tqdmimport osos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"def one_hot(x, class_count=10): return torch.eye(class_count)[x, :]class Generator(nn.Module): def __init__(self): super(Generator, self).__init__() # 两个128 x 7 x 7 cat后依然为256 x 7 x 7 self.linear1 = nn.Sequential( nn.Linear(100, 128 * 7 * 7), nn.ReLU(), nn.BatchNorm1d(128 * 7 * 7), ) self.linear2 = nn.Sequential( nn.Linear(10, 128 * 7 * 7), nn.ReLU(), nn.BatchNorm1d(128 * 7 * 7), ) self.model = nn.Sequential( # 128 x 7 x 7 nn.ConvTranspose2d(256, 128, kernel_size=3, stride=1, padding=1), nn.ReLU(), nn.BatchNorm2d(128), # 64 x 14 x 14 nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1), nn.ReLU(), nn.BatchNorm2d(64), # 1 x 28 x 28 nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1), nn.Tanh() ) def forward(self, x, c): x = self.linear1(x) x = x.view(-1, 128, 7, 7) c = self.linear2(c) c = c.view(-1, 128, 7, 7) # 256 x 7 x 7 # 在channels方面合并 x = torch.cat([x, c], dim=1) return self.model(x)class Discriminator(nn.Module): def __init__(self): super(Discriminator, self).__init__() # input: 1 x 28 x 28 + 10 condition self.linear = nn.Sequential( nn.Linear(10, 1 * 28 * 28), nn.ReLU() ) self.model = nn.Sequential( nn.Conv2d(2, 64, kernel_size=3, stride=2), nn.LeakyReLU(), nn.Dropout(p=0.3), nn.Conv2d(64, 128, kernel_size=3, stride=2), nn.LeakyReLU(), nn.Dropout(p=0.3), nn.BatchNorm2d(128), ) self.fc = nn.Sequential( nn.Linear(128 * 6 * 6, 1), nn.Sigmoid() ) def forward(self, x, c): c = self.linear(c) c = c.view(-1, 1, 28, 28) # 2 x 28 x 28 x = torch.cat([x, c], dim=1) x = self.model(x) x = x.view(-1, 128 * 6 * 6) x = self.fc(x) return xtransform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize(mean=0.5, std=0.5)])dataset = torchvision.datasets.MNIST(\"./data\", train=True, transform=transform, download=True, target_transform=one_hot)dataloader = data.DataLoader(dataset, batch_size=512, shuffle=True)device = \"cuda\" if torch.cuda.is_available() else \"cpu\"gen = Generator().to(device)disc = Discriminator().to(device)loss_fn = torch.nn.BCELoss()opt_g = torch.optim.RMSprop(gen.parameters(), lr=0.0001)opt_d = torch.optim.Adam(disc.parameters(), lr=0.0001)num_epochs = 201writer_g = SummaryWriter(\"/root/tf-logs/g\")writer_d = SummaryWriter(\"/root/tf-logs/d\")noise_seed = torch.randn(16, 100, device=device)# 16个0-10之间的随机整数label_seed = torch.randint(0, 10, size=(16,))print(f\"label seed: {label_seed}\")print(type(label_seed))label_seed_onehot = one_hot(label_seed).to(device)print(f\"label_seed: {label_seed}\")for epoch in range(num_epochs): D_epoch_loss = 0 G_epoch_loss = 0 count = len(dataset) loop = tqdm(dataloader, leave=True, desc=f\"Epoch: {epoch}/{num_epochs}\") for step, (img, label) in enumerate(loop): img = img.to(device) label = label.to(device) size = img.shape[0] random_seed = torch.randn(size, 100, device=device) # 训练判别器 opt_d.zero_grad() # 真实图片放入判别器中 real_output = disc(img, label) d_real_loss = loss_fn(real_output, torch.ones_like(real_output, device=device)) # 生成图像并放入判别器中 gen_img = gen(random_seed, label) fake_output = disc(gen_img.detach(), label) d_fake_loss = loss_fn(fake_output, torch.zeros_like(fake_output, device=device)) d_loss = (d_real_loss + d_fake_loss) / 2 d_loss.backward() opt_d.step() # 训练生成器 opt_g.zero_grad() fake_output = disc(gen_img, label) g_loss = loss_fn(fake_output, torch.ones_like(fake_output, device=device)) g_loss.backward() opt_g.step() with torch.no_grad(): D_epoch_loss += d_loss.item() G_epoch_loss += g_loss.item() loop.set_postfix(G_loss=f\"{np.round(G_epoch_loss, 2)}\", D_loss=f\"{np.round(D_epoch_loss, 2)}\") with torch.no_grad(): D_epoch_loss /= count G_epoch_loss /= count writer_g.add_scalar(\"loss\", G_epoch_loss, epoch) writer_d.add_scalar(\"loss\", D_epoch_loss, epoch) if epoch % 20 == 0: with torch.no_grad(): gen_img = gen(noise_seed, label_seed_onehot) writer_g.add_images(\"gen mnist\", gen_img, epoch)torch.save(gen.state_dict(), \"./gen.pth\") 5. 训练结果 5. 参考资料 https://arxiv.org/abs/1411.1784 https://blog.csdn.net/qq_41647438/article/details/103007057 https://blog.csdn.net/xjp_xujiping/article/details/102719363 https://zhuanlan.zhihu.com/p/510346635 https://www.jianshu.com/p/39c57e9a6630","link":"/2022/10/12/GAN-Condition-GAN/"},{"title":"Git常见问题-OpenSSL SSL_read Connection was reset","text":"1git config --global http.sslVerify \"false\"","link":"/2022/10/12/Git%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98-OpenSSL-SSL-read-Connection-was-reset/"},{"title":"Matplotlib常用方法","text":"Matplotlib常用方法1. Pyplot1.1 根据xy数据绘制默认图像1234567import matplotlib.pyplot as pltimport numpy as npy = np.random.randint(10, 20, size=10)plt.plot(y)plt.show() 1.2 更改绘图标记1.2.1 常用的绘图标记 绘图标记 颜色字符 b 蓝色 m 洋红色 g 绿色 y 黄色 r 红色 k 黑色 w 白色 c 青绿色 #FFD700 金色 线型参数 - 实线 – 破折线 -. 点划线 : 虚线 标记字符 . 点标记 , 像素标记 o 实心圈标记 v 倒三角标记 ^ 上三角标记 &gt; 右三角标记 &lt; 左三角标记 1.2.2 样例123456plt.plot(x, y, 'g:v')plt.show()# 只绘制点，不绘制线plt.plot(x, y, 'gv')plt.show() 2. 轴标签和标题2.1 设置轴标题1234plt.xlabel(\"x\", fontsize=20)plt.ylabel(\"y\", fontsize=20)plt.plot(x, y, 'g:v')plt.show() 2.2 设置标题12345plt.title(\"title\", fontsize=30)plt.xlabel(\"x\", fontsize=20)plt.ylabel(\"y\", fontsize=20)plt.plot(x, y, 'g:v')plt.show() 3. 网格线12345678910plt.title(\"title\", fontsize=30)plt.xlabel(\"x\", fontsize=20)plt.ylabel(\"y\", fontsize=20)plt.plot(x, y, 'g:v')# axis: 只设置x轴上的网格线# color: 设置网格线的颜色# linestyle: 设置网格线的样式# linewidth: 设置网格线的间隔plt.grid(axis='x', color = 'y', linestyle = '--', linewidth = 2)plt.show() 4. 绘制多图12345678910111213141516171819202122232425x = np.arange(1, 11, 1)y_1 = np.random.randint(10, 20, size=10)y_2 = np.random.randint(10, 20, size=10)y_3 = np.random.randint(15, 25, size=10)y_4 = np.random.randint(15, 30, size=10)plt.subplot(2, 2, 1)plt.plot(x, y_1)plt.title(\"plot 1\")plt.subplot(2, 2, 2)plt.plot(x, y_2)plt.title(\"plot 2\")plt.subplot(2, 2, 3)plt.plot(x, y_3)plt.title(\"plot 3\")plt.subplot(2, 2, 4)plt.plot(x, y_4)plt.title(\"plot 4\")plt.suptitle(\"subplot demo\")plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.5, hspace=0.5)plt.show() 5. 显示中文1234567891011121314# 支持中文plt.rcParams['font.sans-serif'] = ['SimHei'] # 用来正常显示中文标签plt.rcParams['axes.unicode_minus'] = False # 用来正常显示负号plt.title(\"图表\", fontsize=30)plt.xlabel(\"x\", fontsize=20)plt.ylabel(\"y\", fontsize=20)plt.plot(x, y, 'g:v')# axis: 只设置x轴上的网格线# color: 设置网格线的颜色# linestyle: 设置网格线的样式# linewidth: 设置网格线的间隔plt.grid(axis='x', color = 'y', linestyle = '--', linewidth = 2)plt.show() 6. 常用的图表6.1 散点图1234567plt.title(\"散点图\", fontsize=30)# color: 设置点的颜色# alpha: 设置点的透明度# s: 设置点的大小plt.scatter(x, y_1, color='y')plt.scatter(x, y_2, color='r', alpha=0.5, s=500)plt.show() 6.2. 柱形图123456789plt.title(\"柱形图\", fontsize=30)plt.xlabel(\"x\", fontsize=20)plt.ylabel(\"y\", fontsize=20)# 纵向的柱形图plt.bar(x, y, color=['y', 'r'], width=0.5)# 横向的柱形图# plt.barh(x, y, color=['y', 'r'], height=0.5)plt.show() 6.3. 饼图123456789plt.title(\"饼图\", fontsize=30)y = np.random.randint(10, 20, size=5)plt.pie(y, labels=['A', 'B', 'C', 'D', 'E'], # 设置饼图标签 colors=['k', 'r', 'y', 'm', 'c'], # 设置饼图颜色 explode=(0, 0.3, 0, 0, 0), # 第二部分突出显示，值越大，距离中心越远 autopct='%.2f%%', # 格式化输出百分比)plt.show()","link":"/2022/10/12/Matplotlib%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95/"},{"title":"YOLOV5-ONNX-Flask","text":"YOLOV5-ONNX-Flask项目部署 Pytorch导出ONNX模型可以参考： https://elijah12138.github.io/post/YOLOV5-Pytorch 1. 下载项目到本地 github地址: 2. templates文件夹保存网页信息index.html 123456789&lt;html&gt; &lt;head&gt; &lt;title&gt;YOLOV5-ONNX-Flask项目部署&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;YOLOV5-ONNX-Flask项目部署&lt;/h1&gt; &lt;img src=\"{{ url_for('video_feed') }}\" width=\"960px\" height=\"720px\"/&gt; &lt;/body&gt;&lt;/html&gt; 3. class.names文件夹下保存待检测的类别1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980personbicyclecarmotorcycleairplanebustraintruckboattraffic lightfire hydrantstop signparking meterbenchbirdcatdoghorsesheepcowelephantbearzebragiraffebackpackumbrellahandbagtiesuitcasefrisbeeskissnowboardsports ballkitebaseball batbaseball gloveskateboardsurfboardtennis racketbottlewine glasscupforkknifespoonbowlbananaapplesandwichorangebroccolicarrothot dogpizzadonutcakechaircouchpotted plantbeddining tabletoilettvlaptopmouseremotekeyboardcell phonemicrowaveoventoastersinkrefrigeratorbookclockvasescissorsteddy bearhair driertoothbrush 4. v5_dnn.py 保存调用ONNX模型进行检测的方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165import cv2import numpy as npclass yolov5(): def __init__(self, modelpath, confThreshold=0.5, nmsThreshold=0.5, objThreshold=0.5): with open('class.names', 'rt') as f: self.classes = f.read().rstrip('\\n').split('\\n') self.num_classes = len(self.classes) if modelpath.endswith('6.onnx'): self.inpHeight, self.inpWidth = 1280, 1280 anchors = [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]] self.stride = np.array([8., 16., 32., 64.]) else: self.inpHeight, self.inpWidth = 640, 640 anchors = [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]] self.stride = np.array([8., 16., 32.]) self.nl = len(anchors) self.na = len(anchors[0]) // 2 self.grid = [np.zeros(1)] * self.nl self.anchor_grid = np.asarray(anchors, dtype=np.float32).reshape(self.nl, -1, 2) self.net = cv2.dnn.readNet(modelpath) self.confThreshold = confThreshold self.nmsThreshold = nmsThreshold self.objThreshold = objThreshold self._inputNames = '' def resize_image(self, srcimg, keep_ratio=True, dynamic=False): top, left, newh, neww = 0, 0, self.inpWidth, self.inpHeight if keep_ratio and srcimg.shape[0] != srcimg.shape[1]: hw_scale = srcimg.shape[0] / srcimg.shape[1] if hw_scale &gt; 1: newh, neww = self.inpHeight, int(self.inpWidth / hw_scale) img = cv2.resize(srcimg, (neww, newh), interpolation=cv2.INTER_AREA) if not dynamic: left = int((self.inpWidth - neww) * 0.5) img = cv2.copyMakeBorder(img, 0, 0, left, self.inpWidth - neww - left, cv2.BORDER_CONSTANT, value=(114, 114, 114)) # add border else: newh, neww = int(self.inpHeight * hw_scale), self.inpWidth img = cv2.resize(srcimg, (neww, newh), interpolation=cv2.INTER_AREA) if not dynamic: top = int((self.inpHeight - newh) * 0.5) img = cv2.copyMakeBorder(img, top, self.inpHeight - newh - top, 0, 0, cv2.BORDER_CONSTANT, value=(114, 114, 114)) else: img = cv2.resize(srcimg, (self.inpWidth, self.inpHeight), interpolation=cv2.INTER_AREA) return img, newh, neww, top, left def _make_grid(self, nx=20, ny=20): xv, yv = np.meshgrid(np.arange(ny), np.arange(nx)) return np.stack((xv, yv), 2).reshape((-1, 2)).astype(np.float32) def preprocess(self, img): img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) img = img.astype(np.float32) / 255.0 return img def postprocess(self, frame, outs, padsize=None): frameHeight = frame.shape[0] frameWidth = frame.shape[1] newh, neww, padh, padw = padsize ratioh, ratiow = frameHeight / newh, frameWidth / neww # Scan through all the bounding boxes output from the network and keep only the # ones with high confidence scores. Assign the box's class label as the class with the highest score. confidences = [] boxes = [] classIds = [] for detection in outs: if detection[4] &gt; self.objThreshold: scores = detection[5:] classId = np.argmax(scores) confidence = scores[classId] * detection[4] if confidence &gt; self.confThreshold: center_x = int((detection[0] - padw) * ratiow) center_y = int((detection[1] - padh) * ratioh) width = int(detection[2] * ratiow) height = int(detection[3] * ratioh) left = int(center_x - width * 0.5) top = int(center_y - height * 0.5) confidences.append(float(confidence)) boxes.append([left, top, width, height]) classIds.append(classId) # Perform non maximum suppression to eliminate redundant overlapping boxes with # lower confidences. indices = cv2.dnn.NMSBoxes(boxes, confidences, self.confThreshold, self.nmsThreshold) for i in indices: box = boxes[i] left = box[0] top = box[1] width = box[2] height = box[3] frame = self.drawPred(frame, classIds[i], confidences[i], left, top, left + width, top + height) return frame def drawPred(self, frame, classId, conf, left, top, right, bottom): cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), thickness=4) label = '%.2f' % conf label = '%s:%s' % (self.classes[classId], label) # Display the label at the top of the bounding box labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1) top = max(top, labelSize[1]) # cv.rectangle(frame, (left, top - round(1.5 * labelSize[1])), (left + round(1.5 * labelSize[0]), top + baseLine), (255,255,255), cv.FILLED) cv2.putText(frame, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), thickness=2) return frame def detect(self, srcimg): img, newh, neww, padh, padw = self.resize_image(srcimg) blob = cv2.dnn.blobFromImage(img, scalefactor=1 / 255.0, swapRB=True) # blob = cv2.dnn.blobFromImage(self.preprocess(img)) # Sets the input to the network self.net.setInput(blob, self._inputNames) # Runs the forward pass to get output of the output layers outs = self.net.forward(self.net.getUnconnectedOutLayersNames())[0].squeeze(axis=0) # inference output row_ind = 0 for i in range(self.nl): h, w = int(self.inpHeight / self.stride[i]), int(self.inpWidth / self.stride[i]) length = int(self.na * h * w) if self.grid[i].shape[2:4] != (h, w): self.grid[i] = self._make_grid(w, h) outs[row_ind:row_ind + length, 0:2] = (outs[row_ind:row_ind + length, 0:2] * 2. - 0.5 + np.tile( self.grid[i], (self.na, 1))) * int(self.stride[i]) outs[row_ind:row_ind + length, 2:4] = (outs[row_ind:row_ind + length, 2:4] * 2) ** 2 * np.repeat( self.anchor_grid[i], h * w, axis=0) row_ind += length srcimg = self.postprocess(srcimg, outs, padsize=(newh, neww, padh, padw)) return srcimgif __name__ == \"__main__\": modelpath = 'yolov5n.onnx' confThreshold = 0.3 nmsThreshold = 0.5 objThreshold = 0.3 yolonet = yolov5(modelpath, confThreshold=confThreshold, nmsThreshold=nmsThreshold, objThreshold=objThreshold) winName = 'Deep learning object detection in OpenCV' cv2.namedWindow(winName, 0) cap = cv2.VideoCapture(0) cap.set(3, 960) # set video width cap.set(4, 780) # set video height while True: hasMoreFrame, frame = cap.read() if hasMoreFrame == True: srcimg = yolonet.detect(frame) cv2.imshow(winName, srcimg) k = cv2.waitKey(20) # q键退出 if (k &amp; 0xff == ord('q')): break cap.release() cv2.destroyAllWindows() 5. app.py保存程序启动信息1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950from operator import nefrom flask import Flask, render_template, Responsefrom v5_dnn import *import timefrom cv2 import getTickCount, getTickFrequencyclass VideoCamera(object): def __init__(self): self.video = cv2.VideoCapture(0) def __del__(self): self.video.release() def get_frame(self): success, image = self.video.read() return imageapp = Flask(__name__)@app.route('/') # 主页def index(): return render_template('index.html')def v5_dnn(camera): modelpath = 'yolov5n.onnx' confThreshold = 0.3 nmsThreshold = 0.5 objThreshold = 0.3 yolonet = yolov5(modelpath, confThreshold=confThreshold, nmsThreshold=nmsThreshold, objThreshold=objThreshold) while True: start = time.clock() frame = camera.get_frame() yolonet.detect(frame) end = time.clock() fps = 1. / (end - start) cv2.putText(frame, \"fps= %.2f\" % (fps), (0, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2) ret, jpeg = cv2.imencode('.jpg', frame) frame = jpeg.tobytes() yield (b'--frame\\r\\n' b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n\\r\\n')@app.route('/video_feed')def video_feed(): return Response(v5_dnn(VideoCamera()), mimetype='multipart/x-mixed-replace; boundary=frame')if __name__ == '__main__': app.run(host='0.0.0.0', debug=True, port=5000)","link":"/2022/10/12/YOLOV5-ONNX-Flask/"},{"title":"YOLOV5-Pytorch(VOC数据集格式)","text":"YOLOV5-Pytorch(VOC数据集格式)1. 下载官方代码并进行测试1.1 克隆github项目到本地12# 已更新到v6.1版本git clone https://github.com/ultralytics/yolov5.git 1.2 安装所需库1pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirements.txt 测试环境: python: 3.7.2 torch: 1.8.1+cu101 torchvision: 0.9.1+cu101 tensorboard: 2.9.1 1.3 下载预训练权重文件 创建一个文件夹weights，用于保存下载的权重文件： 官网下载地址： https://github.com/ultralytics/yolov5/releases 本地如果没有下载权重文件，在运行detect.py时程序也会自动下载权重文件。 1.4 测试检测图片进入yolov5文件夹下，在cmd下运行命令： 12# 以yolov5s.pt为例python detect.py --weights weights/yolov5s.pt --img 640 --conf 0.25 --source bus.jpg --data data/coco.yaml 1.5 测试检测视频1python detect.py --source data/test_video.mp4 --weights weights/yolov5n.pt --data data/coco.yaml 2. 制作自己的数据集2.1 重命名图片123456789101112131415161718192021222324from hashlib import newimport osdef rename_files(path): filelist = os.listdir(path) filelist.sort() total_num = len(filelist) print(\"total number: {}\".format(total_num)) i = 1 for item in filelist: if item.endswith('.jpg'): original_name = os.path.join(path, item) try: new_name = os.path.join(path, str(i).zfill(4) + \".jpg\") i = i + 1 os.rename(original_name, new_name) print(\"{} rename ----&gt; {}\".format(original_name, new_name)) except Exception as e: print(e) print(\"rename dir fail\")path = \"./data\"rename_files(path) 2.2 缩小图片数据 防止出现图片数据过大，显存溢出的现象。 12345678910111213141516171819202122232425import osfrom PIL import Imagedef resize_images(dirPath, new_dirPath): fileName_list = os.listdir(dirPath) filePath_list = [os.path.join(dirPath, fileName) for fileName in fileName_list] imagePath_list = [filePath for filePath in filePath_list if '.jpg' in filePath] if not os.path.isdir(new_dirPath): os.mkdir(new_dirPath) for imagePath in imagePath_list: image = Image.open(imagePath) width, height = image.size imageName = imagePath.split('\\\\')[-1] save_path = os.path.join(new_dirPath, imageName) if width &gt;= 600 and height &gt;= 600: minification = min(width, height) // 300 #此变量表示缩小倍数 new_width = width // minification new_height = height // minification resized_image = image.resize((new_width, new_height), Image.ANTIALIAS) print('图片{}原来的宽{}, 高{}, --------&gt; 图片缩小后宽{}, 高{}' .format(imageName, width, height, new_width, new_height)) resized_image.save(save_path) else: image.save(save_path)resize_images('data', 'processed_data') 2.3 安装labelImg12345# 安装labelImgpip install labelImg# 启动labelImglabelImg 2.4 标记数据 点击Open Dir选择图片数据所在的文件夹，标记格式选择VOC格式； 按一下w键，即可标记目标数据，选定区域后输入所属的标签； 按住ctrl+s保存标记的信息； 切换图片，标记下一张。 3. 训练数据3.1 修改文件夹结构 在yolov5文件夹下创建VOCdevkit / VOC2007文件夹，VOC2007下面建立两个文件夹：Annotations和JPEGImages。 JPEGImages放所有的训练和测试图片；Annotations放所有的xml标记文件 3.2 预处理训练数据在yolov5文件夹里运行代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138import xml.etree.ElementTree as ETimport pickleimport osfrom os import listdir, getcwdfrom os.path import joinimport randomfrom shutil import copyfile# 根据自己的数据标签修改classes=[\"chicken_head\"]def clear_hidden_files(path): dir_list = os.listdir(path) for i in dir_list: abspath = os.path.join(os.path.abspath(path), i) if os.path.isfile(abspath): if i.startswith(\"._\"): os.remove(abspath) else: clear_hidden_files(abspath)def convert(size, box): dw = 1./size[0] dh = 1./size[1] x = (box[0] + box[1])/2.0 y = (box[2] + box[3])/2.0 w = box[1] - box[0] h = box[3] - box[2] x = x*dw w = w*dw y = y*dh h = h*dh return (x,y,w,h)def convert_annotation(image_id): in_file = open('VOCdevkit/VOC2007/Annotations/%s.xml' %image_id) out_file = open('VOCdevkit/VOC2007/YOLOLabels/%s.txt' %image_id, 'w') tree=ET.parse(in_file) root = tree.getroot() size = root.find('size') w = int(size.find('width').text) h = int(size.find('height').text) for obj in root.iter('object'): difficult = obj.find('difficult').text cls = obj.find('name').text if cls not in classes or int(difficult) == 1: continue cls_id = classes.index(cls) xmlbox = obj.find('bndbox') b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text), float(xmlbox.find('ymin').text), float(xmlbox.find('ymax').text)) bb = convert((w,h), b) out_file.write(str(cls_id) + \" \" + \" \".join([str(a) for a in bb]) + '\\n') in_file.close() out_file.close()wd = os.getcwd()wd = os.getcwd()data_base_dir = os.path.join(wd, \"VOCdevkit/\")if not os.path.isdir(data_base_dir): os.mkdir(data_base_dir)work_sapce_dir = os.path.join(data_base_dir, \"VOC2007/\")if not os.path.isdir(work_sapce_dir): os.mkdir(work_sapce_dir)annotation_dir = os.path.join(work_sapce_dir, \"Annotations/\")if not os.path.isdir(annotation_dir): os.mkdir(annotation_dir)clear_hidden_files(annotation_dir)image_dir = os.path.join(work_sapce_dir, \"JPEGImages/\")if not os.path.isdir(image_dir): os.mkdir(image_dir)clear_hidden_files(image_dir)yolo_labels_dir = os.path.join(work_sapce_dir, \"YOLOLabels/\")if not os.path.isdir(yolo_labels_dir): os.mkdir(yolo_labels_dir)clear_hidden_files(yolo_labels_dir)yolov5_images_dir = os.path.join(data_base_dir, \"images/\")if not os.path.isdir(yolov5_images_dir): os.mkdir(yolov5_images_dir)clear_hidden_files(yolov5_images_dir)yolov5_labels_dir = os.path.join(data_base_dir, \"labels/\")if not os.path.isdir(yolov5_labels_dir): os.mkdir(yolov5_labels_dir)clear_hidden_files(yolov5_labels_dir)yolov5_images_train_dir = os.path.join(yolov5_images_dir, \"train/\")if not os.path.isdir(yolov5_images_train_dir): os.mkdir(yolov5_images_train_dir)clear_hidden_files(yolov5_images_train_dir)yolov5_images_test_dir = os.path.join(yolov5_images_dir, \"val/\")if not os.path.isdir(yolov5_images_test_dir): os.mkdir(yolov5_images_test_dir)clear_hidden_files(yolov5_images_test_dir)yolov5_labels_train_dir = os.path.join(yolov5_labels_dir, \"train/\")if not os.path.isdir(yolov5_labels_train_dir): os.mkdir(yolov5_labels_train_dir)clear_hidden_files(yolov5_labels_train_dir)yolov5_labels_test_dir = os.path.join(yolov5_labels_dir, \"val/\")if not os.path.isdir(yolov5_labels_test_dir): os.mkdir(yolov5_labels_test_dir)clear_hidden_files(yolov5_labels_test_dir)train_file = open(os.path.join(wd, \"yolov5_train.txt\"), 'w')test_file = open(os.path.join(wd, \"yolov5_val.txt\"), 'w')train_file.close()test_file.close()train_file = open(os.path.join(wd, \"yolov5_train.txt\"), 'a')test_file = open(os.path.join(wd, \"yolov5_val.txt\"), 'a')list_imgs = os.listdir(image_dir) # list image filesprobo = random.randint(1, 100)print(\"Probobility: %d\" % probo)for i in range(0,len(list_imgs)): path = os.path.join(image_dir,list_imgs[i]) if os.path.isfile(path): image_path = image_dir + list_imgs[i] voc_path = list_imgs[i] (nameWithoutExtention, extention) = os.path.splitext(os.path.basename(image_path)) (voc_nameWithoutExtention, voc_extention) = os.path.splitext(os.path.basename(voc_path)) annotation_name = nameWithoutExtention + '.xml' annotation_path = os.path.join(annotation_dir, annotation_name) label_name = nameWithoutExtention + '.txt' label_path = os.path.join(yolo_labels_dir, label_name) probo = random.randint(1, 100) print(\"Probobility: %d\" % probo) if(probo &lt; 80): # train dataset if os.path.exists(annotation_path): train_file.write(image_path + '\\n') convert_annotation(nameWithoutExtention) # convert label copyfile(image_path, yolov5_images_train_dir + voc_path) copyfile(label_path, yolov5_labels_train_dir + label_name) else: # test dataset if os.path.exists(annotation_path): test_file.write(image_path + '\\n') convert_annotation(nameWithoutExtention) # convert label copyfile(image_path, yolov5_images_test_dir + voc_path) copyfile(label_path, yolov5_labels_test_dir + label_name)train_file.close()test_file.close() 3.3 修改配置文件 新建文件data/voc-chicken.yaml， 内容如下： 12345678train: ../yolov5/VOCdevkit/images/train/val: ../yolov5/VOCdevkit/images/val/# number of classesnc: 1# class namesnames: ['chicken_head'] 复制models/yolov5s.yaml重命名为models/yolov5s-chicken.yaml 然后修改配置参数： 12# Parametersnc: 1 # number of classes 3.4 训练数据1python train.py --data data/voc-chicken.yaml --cfg models/yolov5s-chicken.yaml --weights weights/yolov5s.pt --epochs 500 --device 0 --batch-size 16 4. 训练数据可视化以及性能统计1234567# 安装Tensorboardpip install tensorboard# 启动Tensorboardtensorboard --logdir=./runs# 启动后，在浏览器输入http://localhost:6006/即可 5. 测试训练效果1python detect.py --weights weights/best.pt --img 640 --conf 0.25 --source pic1.png --data data/voc-chicken.yaml 12# 测试模型效果python val.py --weights weights/best.pt --img 640 --conf 0.25 --data data/voc-chicken.yaml --batch-size 8 6. 使用ONNX格式并进行测试 查看ONNX模型的结构： https://netron.app/ 6.1 将pt模型转换为onnx模型1python export.py --weights weights/best.pt --img 640 --batch 1 --opset 12 6.2 通过OpenCV使用ONNX模型1python detect.py --weights weights/best.onnx --dnn --source pic1.png --img 640 --data data/voc-chicken.yaml","link":"/2022/10/12/YOLOV5-Pytorch-VOC%E6%95%B0%E6%8D%AE%E9%9B%86%E6%A0%BC%E5%BC%8F/"},{"title":"YOLOV5-Pytorch(YOLO数据集格式)","text":"YOLOV5-Pytorch(YOLO数据集格式)1. 下载官方代码并进行测试1.1 克隆github项目到本地12# 已更新到v6.1版本git clone https://github.com/ultralytics/yolov5.git 1.2 安装所需库1pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirements.txt 测试环境: python: 3.7.2 torch: 1.8.1+cu101 torchvision: 0.9.1+cu101 tensorboard: 2.9.1 1.3 下载预训练权重文件 创建一个文件夹weights，用于保存下载的权重文件： 官网下载地址： https://github.com/ultralytics/yolov5/releases 本地如果没有下载权重文件，在运行detect.py时程序也会自动下载权重文件。 1.4 测试检测图片进入yolov5文件夹下，在cmd下运行命令： 12# 以yolov5s.pt为例python detect.py --weights weights/yolov5s.pt --img 640 --conf 0.25 --source bus.jpg --data data/coco.yaml 1.5 测试检测视频1python detect.py --source data/test_video.mp4 --weights weights/yolov5n.pt --data data/coco.yaml 2. 制作自己的数据集2.1 重命名图片123456789101112131415161718192021222324from hashlib import newimport osdef rename_files(path): filelist = os.listdir(path) filelist.sort() total_num = len(filelist) print(\"total number: {}\".format(total_num)) i = 1 for item in filelist: if item.endswith('.jpg'): original_name = os.path.join(path, item) try: new_name = os.path.join(path, str(i).zfill(4) + \".jpg\") i = i + 1 os.rename(original_name, new_name) print(\"{} rename ----&gt; {}\".format(original_name, new_name)) except Exception as e: print(e) print(\"rename dir fail\")path = \"./data\"rename_files(path) 2.2 缩小图片数据 防止出现图片数据过大，显存溢出的现象。 12345678910111213141516171819202122232425import osfrom PIL import Imagedef resize_images(dirPath, new_dirPath): fileName_list = os.listdir(dirPath) filePath_list = [os.path.join(dirPath, fileName) for fileName in fileName_list] imagePath_list = [filePath for filePath in filePath_list if '.jpg' in filePath] if not os.path.isdir(new_dirPath): os.mkdir(new_dirPath) for imagePath in imagePath_list: image = Image.open(imagePath) width, height = image.size imageName = imagePath.split('\\\\')[-1] save_path = os.path.join(new_dirPath, imageName) if width &gt;= 600 and height &gt;= 600: minification = min(width, height) // 300 #此变量表示缩小倍数 new_width = width // minification new_height = height // minification resized_image = image.resize((new_width, new_height), Image.ANTIALIAS) print('图片{}原来的宽{}, 高{}, --------&gt; 图片缩小后宽{}, 高{}' .format(imageName, width, height, new_width, new_height)) resized_image.save(save_path) else: image.save(save_path)resize_images('data', 'processed_data') 2.3 安装labelImg12345# 安装labelImgpip install labelImg# 启动labelImglabelImg 2.4 标记数据 点击Open Dir选择图片数据所在的文件夹，标记格式选择YOLO格式； 按一下w键，即可标记目标数据，选定区域后输入所属的标签； 按住ctrl+s保存标记的信息； 切换图片，标记下一张。 3. 训练数据3.1 修改文件夹结构图片数据文件夹的格式目录 YOLO_data |–pre_data 保存未划分的数据集 |—-processed_data 保存未划分数据集的图片 |—-processed_data_label 保存未划分数据集的标签 |–train 保存训练集的图片和标签信息 |–val 保存验证集的图片和标签信息 |–treain.txt 保存训练集数据的存储路径 |–val.txt 保存验证集数据的存储路径 3.2 划分数据集在yolov5文件夹里运行代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667# _*_ coding: utf-8 _*_import timeimport osimport randomimport shutilfrom tqdm import tqdmdef get_filelist(path): Filelist = [] for home, dirs, files in os.walk(path): for filename in files: Filelist.append(os.path.join(home, filename)) return Filelistdef spilt_dataset(img_path, ann_path, img_train_save_path, ann_train_save_path, img_val_save_path, ann_val_save_path,label_suffix): # 标签格式固定，可根据图片确定对应标签名 imgs = os.listdir(img_path) random.seed(2022) random.shuffle(imgs) train_rate = 0.8 print(\"split data -----------&gt;\") for i, img in enumerate(tqdm(imgs)): ann = img.split('.')[0] + label_suffix # 根据标签类型更改 if i &lt;= int(len(imgs) * train_rate): shutil.copy(img_path + img, img_train_save_path + img) shutil.copy(ann_path + ann, ann_train_save_path + ann) else: shutil.copy(img_path + img, img_val_save_path + img) shutil.copy(ann_path + ann, ann_val_save_path + ann) print(\"create train.txt -----------&gt;\") train_list = get_filelist(img_train_save_path) for filename in enumerate(tqdm(train_list)): if filename[1].endswith(\".jpg\"): train_txt_path = img_train_save_path.rsplit(\"/\", 2)[0] + \"/\" with open(train_txt_path + 'train.txt','a') as f: print(os.getcwd() + '/YOLO_data/' + filename[1].split(\"/\", 2)[2], file=f) print(\"create val.txt -----------&gt;\") val_list = get_filelist(img_val_save_path) for filename in enumerate(tqdm(val_list)): if filename[1].endswith(\".jpg\"): val_txt_path = img_val_save_path.rsplit(\"/\", 2)[0] + \"/\" with open(val_txt_path + 'val.txt','a') as f: print(os.getcwd() + '/YOLO_data/' + filename[1].split(\"/\", 2)[2], file=f) print(time.strftime('%Y-%m-%d %X'))if __name__ == '__main__': img_path = './YOLO_data/pre_data/processed_data/' # 图片路径 ann_path = './YOLO_data/pre_data/processed_data_label/' # 标签路径 img_train_save_path = './YOLO_data/train/' # 训练集保存路径 ann_train_save_path = './YOLO_data/train/' # 训练集标签保存时间 img_val_save_path = './YOLO_data/val/' # 验证集img保存路径 ann_val_save_path = './YOLO_data/val/' # 验证集ann保存路径 if not os.path.exists(img_train_save_path): os.makedirs(img_train_save_path) if not os.path.exists(ann_train_save_path): os.makedirs(ann_train_save_path) if not os.path.exists(img_val_save_path): os.makedirs(img_val_save_path) if not os.path.exists(ann_val_save_path): os.makedirs(ann_val_save_path) label_suffix = '.txt' # 标签后缀 spilt_dataset(img_path, ann_path, img_train_save_path, ann_train_save_path, img_val_save_path, ann_val_save_path, label_suffix) 3.3 修改配置文件 新建文件data/voc-chicken.yaml， 内容如下： 123456789path: E:\\practice\\20220703_yolo\\yolov5_v6_yolo\\chciken_head_datasettrain: trainval: val# number of classesnc: 1# class namesnames: ['chicken_head'] 复制models/yolov5s.yaml重命名为models/yolov5s-chicken.yaml 然后修改配置参数： 12# Parametersnc: 1 # number of classes 3.4 训练数据1python train.py --data data/voc-chicken.yaml --cfg models/yolov5s-chicken.yaml --weights weights/yolov5s.pt --epochs 500 --device 0 --batch-size 16 4. 训练数据可视化以及性能统计1234567# 安装Tensorboardpip install tensorboard# 启动Tensorboardtensorboard --logdir=./runs# 启动后，在浏览器输入http://localhost:6006/即可 5. 测试训练效果1python detect.py --weights weights/best.pt --img 640 --conf 0.25 --source pic1.png --data data/voc-chicken.yaml 12# 测试模型效果python val.py --weights weights/best.pt --img 640 --conf 0.25 --data data/voc-chicken.yaml --batch-size 8 6. 使用ONNX格式并进行测试 查看ONNX模型的结构： https://netron.app/ 6.1 将pt模型转换为onnx模型1python export.py --weights weights/best.pt --img 640 --batch 1 --opset 12 6.2 通过OpenCV使用ONNX模型1python detect.py --weights weights/best.onnx --dnn --source pic1.png --img 640 --data data/voc-chicken.yaml","link":"/2022/10/12/YOLOV5-Pytorch-YOLO%E6%95%B0%E6%8D%AE%E9%9B%86%E6%A0%BC%E5%BC%8F/"},{"title":"ipynb转换格式","text":"jupyter转换格式1. 转换为md格式1jupyter nbconvert --to markdown 文件名.ipynb 2. 转换成pdf格式1jupyter nbconvert --to pdf 文件名.ipynb 3. 转换成html格式1jupyter nbconvert --to html 文件名.ipynb 4. 转换成latex格式1jupyter nbconvert --to letex 文件名.ipynb","link":"/2022/10/12/ipynb%E8%BD%AC%E6%8D%A2%E6%A0%BC%E5%BC%8F/"},{"title":"树莓派 PICO基础教程（基于MicroPython）","text":"树莓派 PICO基础教程（基于MicroPython）1 树莓派 PICO 简介1.1 简介 Raspberry Pi Pico是具有灵活数字接口的低成本，高性能微控制器板。它集成了Raspberry Pi自己的RP2040微控制器芯片，运行速度高达133 MHz的双核Arm Cortex M0 +处理器，嵌入式264KB SRAM和2MB板载闪存以及26个多功能GPIO引脚。对于软件开发，可以使用Raspberry Pi的C / C ++ SDK或MicroPython。^1 1.2 配置 ^2 树莓派 PICO配置 双核 Arm Cortex-M0 + @ 133MHz 2 个 UART、2 个 SPI 控制器和 2 个 I2C 控制器 芯片内置 264KB SRAM 和 2MB 的板载闪存 16 个 PWM 通道 通过专用 QSPI 总线支持最高 16MB 的片外闪存 USB 1.1 主机和设备支持 DMA 控制器 8 个树莓派可编程 I/O（PIO）状态机，用于自定义外围设备支持 30 个 GPIO 引脚，其中 4 个可用作模拟输入 支持 UF2 的 USB 大容量存储启动模式，用于拖放式编程 1.3 引脚图 1.4 尺寸 2 安装2.1 烧录固件 点击 https://micropython.org/download/rp2-pico/rp2-pico-latest.uf2 链接下载UF2文件； 如果连接失效，可以进入 https://www.raspberrypi.org/documentation/rp2040/getting-started/#getting-started-with-micropython官网下载 按住BOOTSEL键不放，将Pico插入电脑的USB串口，电脑上会弹出一个新的U盘文件夹，把刚刚下载的UF2文件拖拽到文件夹中，树莓派 PICO将会自动重启，此时，固件烧录完成。 2.2 安装IDE（Thonny IDE） 进入软件官网 https://thonny.org/下载软件，最好下载最新版的，否则可能不支持树莓派 PICO； 安装Thonny，安装完成后打开Thonny软件，打开工具-&gt;设置-&gt; 解释器，选择MicroPython(Raspberry Pi Pico)解释器，并在串口处选择树莓派PICO的串口号（如果板子已经连接在电脑上，软件一般会自动检测串口号） 重启软件，可以看到软件左下方显示了树莓派PICO中的文件； 如果没有显示左侧文件树的话可以勾选 视图-&gt;文件 2.3 离线运行程序 新建文件，编写完代码后，按住ctrl+s将该文件保存在树莓派PICO上，并命名为main.py(一定要加后缀.py)，下次树莓派PICO通电时便会自动运行main.py中的程序。 3 基础3.01 点亮板载LED灯123456789101112from machine import Pinif __name__ == '__main__': # 构建led对象 # 板载LED灯连接与引脚25相连 # LED = Pin(id, mode, pull) # id:PICO引脚编号 # mode:输入输出方式，有Pin.IN(输入)和Pin.OUT(输出)两种 # pull:上下拉电阻配置，有None(无上下拉电阻)、Pin.PULL_UP(上拉电阻)和Pin.PULL_DOWN(下拉电阻)三种 LED = Pin(25, Pin.OUT) # 高电平点亮 LED.value(1) 3.02 板载LED闪烁1234567891011121314from machine import Pinfrom utime import sleepimport utimeled = Pin(25, Pin.OUT)if __name__ == '__main__': while True: # led点亮 led.value(1) utime.sleep_ms(1000) # led熄灭 led.value(0) utime.sleep_ms(1000) 3.03 LED流水灯 LED发光二极管图片 LED发光二极管正负极区分 一般引脚长的一端为正极，引脚短的为负极 看发光二极管内部，支架大的为负极，支架小的为负极 电路连线图 代码 12345678910111213141516from machine import Pinimport utime# 定义LED引脚数组leds = [Pin(i,Pin.OUT) for i in range(0,5)]if __name__ == '__main__': while True: # 依次点亮 for n in range(0,5): leds[n].value(1) utime.sleep_ms(200) # 依次熄灭 for n in range(0,5): leds[n].value(0) utime.sleep_ms(100) 3.04 按键实验 四角按键图片 四角按键怎么连接 默认按键未按下的情况下，12相连接，34相连接；当按下按键时，1234才相连接。 电路接线图 代码 123456789101112131415161718from machine import Pinimport utime# 配置按键# key = machine.Pin(id, mode, pull)# id:树莓派Pico引脚编号# mode:输入输出方式，有Pin.IN(输入)和Pin.OUT(输出)两种# pull:上下拉电阻配置，有None(无上下拉电阻)、Pin.PULL_UP(上拉电阻)和Pin.PULL_DOWN(下拉电阻)三种key = Pin(0, Pin.IN, Pin.PULL_UP)if __name__ == '__main__': while True: # print(key.value()) if key.value() == 0: # 等待一段时间，防止抖动 utime.sleep_ms(100) if key.value() == 0: print('The button is pressed') 按键消抖可以参考https://baike.baidu.com/item/%E6%8C%89%E9%94%AE%E6%B6%88%E6%8A%96 3.05 外部中断(改进3.04 按键实验) 什么是外部中断 外部中断是单片机实时地处理外部事件的一种内部机制。当某种外部事件发生时，单片机的中断系统将迫使CPU暂停正在执行的程序，转而去进行中断事件的处理；中断处理完毕后．又返回被中断的程序处，继续执行下去。^3 外部中断的作用 节省CPU资源 代码实现 在3.04 按键实验中，检测按键是否被按下采用的是在主程序中写死循环的办法，假如这个按键被按下的频率十分低（一天只有几次被按下），采用死循环的方法将会浪费大量的CPU资源，而采用外部中断的方式检测按键是否被按下将大大节省CPU资源。 123456789101112131415161718192021from machine import Pinimport utime#配置按键key = Pin(0, Pin.IN, Pin.PULL_UP)def external_interrupt(key): # 消除抖动 utime.sleep_ms(100) # 再次判断按键是否被按下 if key.value() == 0: print('The button is pressed')if __name__ == '__main__': # KEY.irq(handler,trigger) # handler:中断执行的回调函数 # trigger:触发中断的方式，分别为Pin.IRQ_FALLING(下降沿触发)、 # Pin.IRQ_RISING(上升沿触发)、Pin.IRQ_LOW_LEVEL(低电平触发)和 # Pin.IRQ_HIGH_LEVEL(高电平触发)四种 # 定义中断，下降沿触发 key.irq(external_interrupt, Pin.IRQ_FALLING) 3.06 定时器中断(改进3.02 板载LED闪烁) 什么是定时器中断 定时器中断是由单片机中的定时器溢出而申请的中断，即设定一个时间，到达这个时间后就会产生中断 代码 通过设置定时器中断使树莓派PICO板载LED每隔两秒闪烁一次 123456789101112131415161718from machine import Pin, Timer# 创建LED对象led=Pin(25, Pin.OUT)# 闪烁回调函数def twinkle(tim): # toggle方法:LED状态翻转 led.toggle()if __name__ == '__main__': # 构建定时器 tim = Timer() # tim.init(period, mode, callback) # period:周期时间(单位为ms) # mode:工作模式，有Timer.ONE_SHOT(执行一次)和Timer.PERIODIC(周期性执行)两种 # callback:定时器中断的回调函数 tim.init(period=2000, mode=Timer.PERIODIC, callback=twinkle) 3.07 PWM 脉冲宽度调制(实现板载LED呼吸灯) 什么是PWM 脉冲宽度调制是一种模拟控制方式，根据相应载荷的变化来调制晶体管基极或MOS管栅极的偏置，来实现晶体管或MOS管导通时间的改变，从而实现开关稳压电源输出的改变。这种方式能使电源的输出电压在工作条件变化时保持恒定，是利用微处理器的数字信号对模拟电路进行控制的一种非常有效的技术。脉冲宽度调制是利用微处理器的数字输出来对模拟电路进行控制的一种非常有效的技术，广泛应用在从测量、通信到功率控制与变换的许多领域中。^4 代码 1234567891011121314151617181920212223from machine import Pin, Timer, PWMimport utimeled = PWM(Pin(25))# 设置频率值led.freq(1000)led_value = 0# led以5%增长/减少的速度变化亮度led_space = 5if __name__ == '__main__': while True: led_value += led_space if led_value &gt;= 100: led_value = 100 led_space = -5 elif led_value &lt;= 0: led_value = 0 led_space = 5 # 设置占空比，需在0-65535之间 led.duty_u16(int(led_value * 500)) utime.sleep_ms(100) 3.08 I2C总线(使用SSD1306 OLED屏幕) I2C总线简介 I2C总线是由Philips公司开发的一种简单、双向二线制同步串行总线。它只需要两根线即可在连接于总线上的器件之间传送信息。I2C由 2 条线组成：SDA（串行数据线）和SCL（串行时钟线），都是双向I/O线。^5 SSD1306 OLED简介 SSD1306是一款带控制器的用于OLED点阵图形显示系统的单片CMOS OLED/PLED驱动器。它由128个SEG（列输出）和64个COM（行输出）组成。该芯片专为共阴极OLED面板设计。 SSD1306内置对比度控制器、显示RAM（GDDRAM）和振荡器，以此减少了外部元件的数量和功耗。该芯片有256级亮度控制。数据或命令由通用微控制器通过硬件选择的6800/8000系通用并行接口、I2C接口或串行外围接口发送。该芯片适用于许多小型便携式应用，如手机副显示屏、MP3播放器和计算器等。^6 电路连线图 代码 ssd1306.py下载地址： https://elijah.lanzoui.com/iJ13fpnq6je 下载完成后，在Thonny软件左侧的文件窗口内找到这个文件，右键点击文件，选择上载到选项，文件即可传输到树莓派PICO上 1234567891011121314151617from machine import SoftI2C, Pin# 导入SSD1306驱动模块from ssd1306 import SSD1306_I2Cif __name__ == '__main__': # 初始化SoftI2C # OLED屏幕的scl连接到树莓派PICO的GPIO0, sda连接到GPIO1 i2c = SoftI2C(scl=Pin(0), sda=Pin(1)) # oled = SSD1306_I2C(width, height, i2c, addr) # width:屏幕宽 # height: 屏幕高 # i2c:已定义的I2C对象 oled = SSD1306_I2C(128, 64, i2c) #OLED显示屏初始化：128*64分辨率,OLED的I2C地址是0x3c # OLED显示的字符串，横坐标和纵坐标 oled.text(\"Hello World!\", 0, 0) # OLED显示 oled.show() 4 传感器程序4.1 温度传感器(DS18B20)DS18B20是常用的数字温度传感器，其输出的是数字信号，具有体积小，硬件开销低，抗干扰能力强，精度高的特点。 测温范围: -55℃～+125℃，固有测温误差1℃ 工作电源: 3.0~5.5V/DC 单总线驱动，只占用一个IO口 1234567891011121314151617181920212223242526import machine, onewire, ds18x20, time, utime# 使用GPIO0口传输数据# 将DS18B20的VCC端连接到树莓派PICO的3V3(OUT)端# 将DS18B20的数据端连接到树莓派PICO的GPIO0口# 将DS18B20的GND端连接到树莓派PICO的GND端pin = machine.Pin(0)sensor = ds18x20.DS18X20(onewire.OneWire(pin))# 扫描是否存在DS18B20设备roms = sensor.scan()print('Found a ds18x20 device')# 获取温度数据def detect_tem(): while True: sensor.convert_temp() for rom in roms: # 打印出温度值 # 第一个打印出来的数值可能不太准确，从第二条数据开始才会显示出正常数据 print(\"{:.3f}\".format(sensor.read_temp(rom))) utime.sleep_ms(2000) # 程序入口if __name__ == '__main__': detect_tem() 4.2 温湿度传感器 DHT22.py文件下载地址： https://elijah.lanzoui.com/iFueapnq6id 文件上传方法参考3.08 I2C总线 4.2.1 DHT11DHT11是一款有已校准数字信号输出的温湿度传感器。 其精度湿度±5%RH， 温度±2℃，量程湿度5-95%RH， 温度0-+50℃。^9 123456789101112131415161718192021222324from machine import Pinfrom DHT22 import DHT22import utimepin = Pin(0,Pin.IN,Pin.PULL_UP)# 创建dht11对象# 将DHT11的VCC端连接到树莓派PICO的3V3(OUT)端# 将DHT11的数据端连接到树莓派PICO的GPIO0口# 将DHT11的GND端连接到树莓派PICO的GND端dht_sensor=DHT22(pin, dht11=True)# 循环函数def detection(): while True: T, H = dht_sensor.read() if T is None: print(\"sensor error\") else: print(\"{}'C {}%\".format(T, H)) utime.sleep_ms(2000)# 程序入口if __name__ == '__main__': detection() 4.2.1 DHT22 DHT22也称AM2302，是一款含有已校准数字信号输出的温湿度复合传感器，湿度量程范围0-99.9%RH，精度±2%RH，而温度量程范围是-40℃-80℃，精度±0.5℃。^10 123456789101112131415161718192021222324from machine import Pinfrom DHT22 import DHT22import utimepin = Pin(0,Pin.IN,Pin.PULL_UP)# 创建dht11对象# 将DHT11的VCC端连接到树莓派PICO的3V3(OUT)端# 将DHT11的数据端连接到树莓派PICO的GPIO0口# 将DHT11的GND端连接到树莓派PICO的GND端dht_sensor=DHT22(pin, dht11=False)# 循环函数def detection(): while True: T, H = dht_sensor.read() if T is None: print(\"sensor error\") else: print(\"{:.2f}'C {:.2f}%\".format(T, H)) utime.sleep_ms(2000)# 程序入口if __name__ == '__main__': detection()","link":"/2022/10/12/%E6%A0%91%E8%8E%93%E6%B4%BE-PICO%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B%EF%BC%88%E5%9F%BA%E4%BA%8EMicroPython%EF%BC%89/"},{"title":"深度学习-自动求导(Pytorch)","text":"深度学习-自动求导(Pytorch)1import torch 创建一个1 * 4的向量 12x = torch.arange(4.0)x, x.shape (tensor([0., 1., 2., 3.]), torch.Size([4])) 标量是0阶张量(一个数)，是1 * 1的； 向量是一阶张量，是1 * n的； 张量可以给出所有坐标间的关系，是n * n的； 张量不能使用backward()，需要转化为标量。 如果需要自动计算梯度，需要设置x.requires_grad_为True 12x.requires_grad_(True)x.grad y = 2 * x * x^T 此时y是一个标量，28 = 2 * (0 * 0 + 1 * 1 + 2 * 2 + 3 * 3) 123# 计算两个张量的点积（内积）y = 2 * torch.dot(x, x)y tensor(28., grad_fn=) 调用backward()自动计算y关于x的梯度 由于x是一个一维的向量，所以y对于x求导就等于4 * x，即4 * [0, 1, 2, 3] = [0, 4, 8, 12] 12y.backward()x.grad tensor([ 0., 4., 8., 12.]) 12# 验证求导过程是否正确x.grad == 4 * x tensor([True, True, True, True]) 默认情况下，pytorch会累积梯度，因此在下一次计算前，需要清除之前保存的值 torch.sum()表示对输入的tensor数据的某一维度求和令y = x.sum()，y表示一个数，因此y对x的求导为1 1234x.grad.zero_()y = x.sum()y.backward()x.grad, y (tensor([1., 1., 1., 1.]), tensor(6., grad_fn=)) 当y不是标量时 例如：y = x * x，此时y是一个向量，所以无法直接使用backward() 要使用backward()有两种方法： 使用y.sum() torch.ones_like(x) y对x求导等于2 * x，即2 * [0, 1, 2, 3] = [0, 2, 4, 6] 12345x.grad.zero_()y = x * xy.sum().backward()# y.backward(torch.ones_like(x))x.grad tensor([0., 2., 4., 6.]) 在运算的过程中，可以使用detach()将部分计算结果移动到图外 此时，对系统来说，u变成了与x无关的量 参考资料 123456x.grad.zero_()y = x * xu = y.detach()z = u * xz.sum().backward()x.grad == u tensor([True, True, True, True])","link":"/2022/10/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC-Pytorch/"},{"title":"语义分割基础","text":"1. 分类 语义分割 类似于二分类，只需要把物体标出来；FCN 实例分割 需要将不同的物体用不同的颜色标注出来；Mask R-CNN 全景分割 需要划分背景；Panoptic FPN 2. 数据集2.1 PASCAL VOCPNG图片，P模式，通道数为1调色板模式：每个像素的数值对应类别索引 2.2 MS COCO记录图像中每一个目标的多边形坐标 3. 常用的指标$n_{ij}$: 类别i被预测成类别j的像素个数；$n_{cls}$: 目标类别个数（包括背景）$t_i= {\\textstyle \\sum_{j}^{} n_{ij}}$: 目标类别i的总像素个数（真实标签） 3.1 Pixel Accuracy(Global Acc)$\\frac{ {\\textstyle \\sum_{i}^{} n_{ii}} }{ {\\textstyle \\sum_{i}^{} t_i} }$预测正确的像素个数与图片总像素个数的比值。 3.2 mean Accuracy$\\frac{1}{n_{cls}} {\\textstyle \\sum_{i}^{} \\frac{n_{ii}}{t_i} }$将每个类别的Accuracy计算出来，然后求和，取平均。对于类别i而言，预测正确的像素个数除以某个类别真实标签的像素总个数。 3.3 mean IOU$\\frac{1}{n_{cls}} {\\textstyle \\sum_{i}^{} \\frac{n_{}ii}{t_i + {\\textstyle \\sum_{j}^{} n_{ji}} - n_{}ii } }$对于每个类别i而言，预测正确的像素个数除以（真实标签元素的个数+预测类别为i的像素个数-预测正确的像素个数） 4. 转置卷积4.1 转置卷积的计算过程假设转置卷积的卷积核大小、步长和填充分别用k、s和p表示 在输入特征图元素间填充s-1行（列），填充的元素值为0； 在输入特征图的四周填充k-p-1行（列），填充的元素值为0； 将卷积核上下左右进行翻转； 做正常的卷积，padding=0，stride=1。 $H_{out} = (H_{in} - 1) \\times stride[0] - 2 \\times padding[0] + dilation[0] \\times (kernelsize[0] - 1) + outpadding[0] + 1$$W_{out} = (W_{in} - 1) \\times stride[1] - 2 \\times padding[1] + dilation[1] \\times (kernelsize[1] - 1) + outpadding[1] + 1$当kernelsize=1，outpadding=0时，公式也可表示为：$H_{out} = (H_{in} - 1) \\times stride[0] - 2 \\times padding[0] + kernelsize[0]$$W_{out} = (W_{in} - 1) \\times stride[1] - 2 \\times padding[1] + kernelsize[1]$ 4.2 图示 5. 召回率（recall）和精度（precision）TP—— 预测为 P （正例）, 预测对了， 本来是正样本，检测为正样本（真阳性）TN—— 预测为 N （负例）, 预测对了， 本来是负样本，检测为负样本（真阴性）FP—— 预测为 P （正例）, 预测错了， 本来是负样本，检测为正样本（假阳性）FN—— 预测为 N （负例）, 预测错了， 本来是正样本，检测为负样本（假阴性） TP+FP+TN+FN：样本总数。TP+FN：实际正样本数。TP+FP：预测结果为正样本的总数，包括预测正确的和错误的。FP+TN：实际负样本数。TN+FN：预测结果为负样本的总数，包括预测正确的和错误的 5.1 召回率(Recall)表示的是样本中的正例有多少被预测正确了（找得全）所有正例中被正确预测出来的比例。用途：用于评估检测器对所有待检测目标的检测覆盖率 针对数据集中的所有正例(TP+FN)而言，模型正确判断出的正例(TP)占数据集中所有正例的比例。FN表示被模型误认为是负例但实际是正例的数据，召回率也叫查全率，以物体检测为例，我们往往把图片中的物体作为正例，此时召回率高代表着模型可以找出图片中更多的物体 $Recall = \\frac{TP}{TP + FN}$ 5.2 精确率(Precision)表示的是预测为正的样本中有多少是真正的正样本（找得对）。预测结果中真正的正例的比例。 用途：用于评估检测器在检测成功基础上的正确率 针对模型判断出的所有正例(TP+FP)而言，其中真正例(TP)占的比例。精确率也叫查准率，还是以物体检测为例，精确率高表示模型检测出的物体中大部分确实是物体，只有少量不是物体的对象被当成物体。 $Precision = \\frac{TP}{TP + FP}$recall 表示在整个检测结果中有用部分占整个数据集有用部分的比重，precision 表示在整个检测结果中有用部分占整个检测结果为有用的比重。 5.3 准确率(Accuracy)模型判断正确的数据(TP+TN)占总数据的比例$Acc = \\frac{TP + TN}{TP + TN + FP + FN}$ 6. 感受野的计算公式感受野：在CNN中，输出的feature map上的一个单元对应输入层上的区域大小$F(i) = F(F(i + 1) - 1) \\times stride + kernelsize$例：feature map: F = 1stride = 1conv 3x3(3): F = ( 1 - 1 ) x 1 + 3 = 3conv3x3(2): F = (3 - 1) x 1 + 3 = 5conv3x3(1): F = (5 - 1) x 1 + 3 = 7 7. 卷积的计算公式7.1 卷积的计算公式$(\\frac{ h + 2 \\times p_h - k_h }{s_h} + 1) \\times (\\frac{ w + 2 \\times p_w - k_w }{ s_w } + 1)$ 7.2 转置卷积的计算公式$[( h - 1 ) \\times s_h - 2 \\times p_h + k_h + outputpadding] \\times [( w - 1 ) \\times s_w - 2 \\times p_w + k_w + outputpadding]$","link":"/2022/10/12/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"Android","slug":"Android","link":"/tags/Android/"},{"name":"databinding","slug":"databinding","link":"/tags/databinding/"},{"name":"Kotlin","slug":"Kotlin","link":"/tags/Kotlin/"},{"name":"okHttp","slug":"okHttp","link":"/tags/okHttp/"},{"name":"Jetpack","slug":"Jetpack","link":"/tags/Jetpack/"},{"name":"Adroid","slug":"Adroid","link":"/tags/Adroid/"},{"name":"CNN","slug":"CNN","link":"/tags/CNN/"},{"name":"VGG","slug":"VGG","link":"/tags/VGG/"},{"name":"Git","slug":"Git","link":"/tags/Git/"},{"name":"GAN","slug":"GAN","link":"/tags/GAN/"},{"name":"CGAN","slug":"CGAN","link":"/tags/CGAN/"},{"name":"Pytorch","slug":"Pytorch","link":"/tags/Pytorch/"},{"name":"Matplotlib","slug":"Matplotlib","link":"/tags/Matplotlib/"},{"name":"DL","slug":"DL","link":"/tags/DL/"},{"name":"ML","slug":"ML","link":"/tags/ML/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"Object detection","slug":"Object-detection","link":"/tags/Object-detection/"},{"name":"Yolo","slug":"Yolo","link":"/tags/Yolo/"},{"name":"ONNX","slug":"ONNX","link":"/tags/ONNX/"},{"name":"Flask","slug":"Flask","link":"/tags/Flask/"},{"name":"jupyter","slug":"jupyter","link":"/tags/jupyter/"},{"name":"MCU","slug":"MCU","link":"/tags/MCU/"},{"name":"树莓派pico","slug":"树莓派pico","link":"/tags/%E6%A0%91%E8%8E%93%E6%B4%BEpico/"},{"name":"MicroPython","slug":"MicroPython","link":"/tags/MicroPython/"},{"name":"语义分割","slug":"语义分割","link":"/tags/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"}],"categories":[{"name":"Android","slug":"Android","link":"/categories/Android/"},{"name":"VGG","slug":"VGG","link":"/categories/VGG/"},{"name":"Git","slug":"Git","link":"/categories/Git/"},{"name":"GAN","slug":"GAN","link":"/categories/GAN/"},{"name":"Matplotlib","slug":"Matplotlib","link":"/categories/Matplotlib/"},{"name":"Object detection","slug":"Object-detection","link":"/categories/Object-detection/"},{"name":"jupyter","slug":"jupyter","link":"/categories/jupyter/"},{"name":"MCU","slug":"MCU","link":"/categories/MCU/"},{"name":"Pytorch","slug":"Pytorch","link":"/categories/Pytorch/"},{"name":"语义分割","slug":"语义分割","link":"/categories/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"}],"pages":[{"title":"关于我","text":"AI新手程序员","link":"/about/index.html"},{"title":"日常","text":"","link":"/pictures/index.html"}]}